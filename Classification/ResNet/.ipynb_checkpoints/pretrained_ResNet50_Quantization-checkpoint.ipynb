{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f633d99-00cd-46f3-9bf0-7743f915bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageNet\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.quantization import get_default_qconfig, prepare, convert\n",
    "from torch.quantization import fuse_modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd8788-1a78-4982-b39a-630d0a560d26",
   "metadata": {},
   "source": [
    "### CIFAR10 데이터 갖고오기 및 DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26dc363-97df-46f1-a149-4c085a17f2eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The archive ILSVRC2012_devkit_t12.tar.gz is not present in the root directory or is corrupted. You need to download it externally and place it in /path/to/imagenet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      2\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m256\u001b[39m),\n\u001b[0;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mCenterCrop(\u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m                          std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]),\n\u001b[0;32m      7\u001b[0m ])\n\u001b[1;32m----> 9\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/path/to/imagenet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:53\u001b[0m, in \u001b[0;36mImageNet.__init__\u001b[1;34m(self, root, split, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(root)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m=\u001b[39m verify_str_arg(split, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_archives\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m wnid_to_classes \u001b[38;5;241m=\u001b[39m load_meta_file(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:66\u001b[0m, in \u001b[0;36mImageNet.parse_archives\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_archives\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, META_FILE)):\n\u001b[1;32m---> 66\u001b[0m         \u001b[43mparse_devkit_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_folder):\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:147\u001b[0m, in \u001b[0;36mparse_devkit_archive\u001b[1;34m(root, file)\u001b[0m\n\u001b[0;32m    144\u001b[0m     file \u001b[38;5;241m=\u001b[39m archive_meta[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    145\u001b[0m md5 \u001b[38;5;241m=\u001b[39m archive_meta[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 147\u001b[0m \u001b[43m_verify_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_tmp_dir() \u001b[38;5;28;01mas\u001b[39;00m tmp_dir:\n\u001b[0;32m    150\u001b[0m     extract_archive(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file), tmp_dir)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:103\u001b[0m, in \u001b[0;36m_verify_archive\u001b[1;34m(root, file, md5)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file), md5):\n\u001b[0;32m     99\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe archive \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not present in the root directory or is corrupted. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to download it externally and place it in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m     )\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(file, root))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The archive ILSVRC2012_devkit_t12.tar.gz is not present in the root directory or is corrupted. You need to download it externally and place it in /path/to/imagenet."
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_dataset = ImageNet(root=\"./pretrained_ResNet50_testdata\", split=\"val\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e97dcd-3c75-41e7-b416-4bacf668e04a",
   "metadata": {},
   "source": [
    "### 양자화 할 pretrained ResNet50 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8d204-da60-4ec1-bd1c-ae3948d525d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "model.to('cpu')  # 정적 양자화는 CPU에서 실행 (fbgemm backend를 사용하여 cpu에 최적화된 양자화를 하기 위해)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae07dd08-465f-4c09-a7c7-f982ccfc9585",
   "metadata": {},
   "source": [
    "### 양자화 진행을 위한 계층 합치기(fusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e9a86-b9ce-4c79-a2bc-00fbf20f6a69",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e2e61-eac9-4bc1-9338-b5d8757bbc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fuse_model = lambda: None  # model.fuse_model 함수 호출시에도 오류가 나지 않도록 더미 함수 생성 (monkey patching)\n",
    "qconfig = get_default_qconfig('fbgemm') # fbgemm: x86 CPU (Intel/AMD) || qnnpack: ARM CPU (모바일 기기 등)\n",
    "model.qconfig = qconfig\n",
    "\n",
    "fused_model = model\n",
    "\n",
    "def fuse_resnet50_modules(model):\n",
    "    # Top-level conv fuse\n",
    "    fuse_modules(model, [['conv1', 'bn1', 'relu']], inplace=True)\n",
    "\n",
    "    # Layer1~4: Bottleneck blocks fuse\n",
    "    for layer_name in ['layer1', 'layer2', 'layer3', 'layer4']:\n",
    "        layer = getattr(model, layer_name)\n",
    "        for bottleneck in layer:\n",
    "            fuse_modules(bottleneck, [['conv1', 'bn1']], inplace=True)\n",
    "            fuse_modules(bottleneck, [['conv2', 'bn2']], inplace=True)\n",
    "            fuse_modules(bottleneck, [['conv3', 'bn3']], inplace=True) # relu 는 블록의 마지막에 단독으로 존재하므로 포함하지 않음. by ChatGPT\n",
    "\n",
    "            if hasattr(bottleneck, 'downsample') and bottleneck.downsample is not None:\n",
    "                fuse_modules(bottleneck.downsample, [['0', '1']], inplace=True)\n",
    "\n",
    "#torch.quantization.convert() 호출시 Linear 계층은 자동으로 양자화 됨.\n",
    "fuse_resnet50_modules(fused_model)\n",
    "\n",
    "fused_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4abc15-78f3-4cf4-bfaf-a4238cefef5c",
   "metadata": {},
   "source": [
    "### 모델 양자화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281eb8ec-1ce9-456e-b78e-586e2db26381",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_model = prepare(fused_model)\n",
    "\n",
    "def calibrate(model, loader): # 정적 양자화에서 필요한 scale/zero_point를 추정하기 위해 calibration 진행\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(loader):\n",
    "            if i > 10: break  # 일부 input 만 사용\n",
    "            model(images)\n",
    "\n",
    "calibrate(prepared_model, test_loader)\n",
    "\n",
    "quantized_model = convert(prepared_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5641c-7767-40b4-ae6e-139d90297201",
   "metadata": {},
   "source": [
    "### 양자화 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e49303-abbb-4292-bd4e-266676a01160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    end = time.time()\n",
    "    acc = 100 * correct / total\n",
    "    latency = (end - start) / len(loader)\n",
    "    return acc, latency\n",
    "\n",
    "acc, latency = evaluate(model, test_loader)\n",
    "\n",
    "print(f\"[PyTorch] Accuracy: {acc:.2f}%, Latency: {latency:.4f}s per batch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
