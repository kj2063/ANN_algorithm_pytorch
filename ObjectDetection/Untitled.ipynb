{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde6c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077e5acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'door',\n",
       " 1: 'cabinetDoor',\n",
       " 2: 'refrigeratorDoor',\n",
       " 3: 'window',\n",
       " 4: 'chair',\n",
       " 5: 'table',\n",
       " 6: 'cabinet',\n",
       " 7: 'couch',\n",
       " 8: 'openedDoor',\n",
       " 9: 'pole'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "import random\n",
    "\n",
    "data_config = open('../data/IndoorObjectsDetection/data.yaml')\n",
    "\n",
    "data_info = yaml.load(data_config, Loader=yaml.FullLoader)\n",
    "\n",
    "train_data_path = data_info['train_data_path']\n",
    "val_data_path = data_info['val_data_path']\n",
    "test_data_path = data_info['test_data_path']\n",
    "\n",
    "train_labels_path = data_info['train_labels_path']\n",
    "val_labels_path = data_info['val_labels_path']\n",
    "test_labels_path = data_info['test_labels_path']\n",
    "\n",
    "target_list = data_info['names']\n",
    "target_dict = dict(zip(range(len(target_list)), target_list))\n",
    "\n",
    "target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a21b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 224\n",
    "\n",
    "resnet18 = torchvision.models.resnet18(pretrained = False)\n",
    "layers = [m for m in resnet18.children()]\n",
    "\n",
    "# 마지막 2층인 average pooling & fully connected layer 은 back bone으로 사용하지 않음\n",
    "test_net = nn.Sequential(*layers[:-2]) \n",
    "\n",
    "temp_x = torch.randn(1,3,IMAGE_SIZE,IMAGE_SIZE)\n",
    "temp_y = test_net(temp_x)\n",
    "\n",
    "\n",
    "print(type(temp_x))\n",
    "print(temp_x.shape)\n",
    "print(temp_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dcc8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv1_RESNET(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_bboxes = 2\n",
    "        self.grid_size = 7\n",
    "        \n",
    "        resnet18 = torchvision.models.resnet18(pretrained = False)\n",
    "        layers = [m for m in resnet18.children()]\n",
    "        \n",
    "        self.backbone = nn.Sequential(*layers[:-2])\n",
    "        \n",
    "        self.neck = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1024, out_channels=5*self.num_bboxes+self.num_classes, kernel_size=1, padding=0, bias=False),\n",
    "            nn.AdaptiveAvgPool2d(output_size=(self.grid_size, self.grid_size))\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        out = self.neck(out)\n",
    "        out = self.head(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f925b4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLOv1_RESNET(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (neck): Sequential(\n",
       "    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): Conv2d(1024, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = len(target_list)\n",
    "model = YOLOv1_RESNET(num_classes = NUM_CLASSES)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02020a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detection_dataset():\n",
    "    def __init__(self, data_dir, phase, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.phase = phase\n",
    "        self.image_files = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        for fn in os.listdir(os.path.join(self.data_dir, phase, 'images')):\n",
    "            bboxes, class_ids = self.get_label(fn)\n",
    "                                \n",
    "            if(fn.endswith(\"jpg\") and bboxes.size != 0 and class_ids.size != 0):\n",
    "                self.image_files.append(fn)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename, image = self.get_image(index)\n",
    "        bboxes, class_ids = self.get_label(filename)\n",
    "        \n",
    "        if self.transform: \n",
    "            transformed_data = self.transform(image=image, bboxes=bboxes, class_ids=class_ids)\n",
    "            image = transformed_data['image']\n",
    "            bboxes = np.array(transformed_data['bboxes'])\n",
    "            class_ids = np.array(transformed_data['class_ids'])\n",
    "        else:\n",
    "            #transform 을 하지 않을경우 reshape to (C,W,H)\n",
    "            image = torch.Tensor(image).permute(2,0,1)\n",
    "        \n",
    "        target = np.concatenate((bboxes, class_ids[:, np.newaxis]), axis=1)\n",
    "        return image, target, filename\n",
    "    \n",
    "    def get_image(self, index):\n",
    "        filename = self.image_files[index]\n",
    "        image_path = os.path.join(self.data_dir, self.phase, 'images', filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        return filename, image\n",
    "    \n",
    "    \n",
    "    def get_label(self, filename):\n",
    "        image_id = filename.split('.')[0]\n",
    "        label_file_path = os.path.join(self.data_dir, self.phase, 'labels') + '/' + image_id + '.txt'\n",
    "        try:\n",
    "            bbox_df = pd.read_csv(label_file_path, sep=' ', header=None)\n",
    "            \n",
    "            bboxes = np.asarray(bbox_df[[1,2,3,4]])\n",
    "            class_ids = np.asarray(bbox_df[0])\n",
    "            \n",
    "        except Exception as e:\n",
    "            bboxes = np.array([])\n",
    "            class_ids = np.array([])\n",
    "            \n",
    "            \n",
    "        return bboxes, class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fcf18cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose([\n",
       "  Resize(always_apply=False, p=1, height=448, width=448, interpolation=1),\n",
       "  Normalize(always_apply=False, p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0),\n",
       "  ToTensorV2(always_apply=True, p=1.0, transpose_mask=False),\n",
       "], p=1.0, bbox_params={'format': 'yolo', 'label_fields': ['class_ids'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params=None, additional_targets={}, is_check_shapes=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "IMAGE_SIZE = 448\n",
    "\n",
    "\"\"\"\n",
    "    when you use yolo format bbox param\n",
    "    need to add logic in albumentations/augmentations/bbox_utils.py - check_bbox() method\n",
    "    to make bbox boundery in [0,1]\n",
    "    \n",
    "    -------------------\n",
    "    bbox=list(bbox)\n",
    "    \n",
    "    for i in range(4):\n",
    "      if (bbox[i]<0) :\n",
    "        bbox[i]=0\n",
    "      elif (bbox[i]>1) :\n",
    "        bbox[i]=1\n",
    "    \n",
    "    bbox=tuple(bbox)\n",
    "    --------------------\n",
    "\"\"\"\n",
    "\n",
    "#mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225) -> imageNet 데이터셋에 기반한 계산된 수치 \n",
    "transform = A.Compose([\n",
    "        A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    ")\n",
    "\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec0c4207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7678699f1d04397a1d15a8381ce3dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=866), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.utils import make_grid\n",
    "from my_util import set_bounding_boxes, set_bounding_box, get_random_color_dict\n",
    "from ipywidgets import interact\n",
    "\n",
    "transformed_train_dataset = Detection_dataset(data_dir='../data/IndoorObjectsDetection', phase=\"train\", transform=transform)\n",
    "\n",
    "@interact(index=(0, len(transformed_train_dataset)-1))\n",
    "def show_transformed_image(index=0):\n",
    "    img, target, filename = transformed_train_dataset[index]\n",
    "    \n",
    "    np_image = make_grid(img, normalize=True).permute(1,2,0).numpy()\n",
    "    np_image_unit8 = (np_image*255).astype(np.uint8)\n",
    "    \n",
    "    res = set_bounding_boxes(np_image_unit8, target[:,0:4], 'yolo', target[:,4].astype(int), target_dict, get_random_color_dict(target_dict))\n",
    "    plt.imshow(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fe4aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "data_dir = '../data/IndoorObjectsDetection'\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image_list = []\n",
    "    target_list = []\n",
    "    filename_list = []\n",
    "    \n",
    "    for a,b,c in batch:\n",
    "        image_list.append(a)\n",
    "        target_list.append(b)\n",
    "        filename_list.append(c)\n",
    "        \n",
    "    return torch.stack(image_list, dim=0), target_list, filename_list\n",
    "\n",
    "def train_valid_dataloader(data_dir, batch_size=4, transform=None):\n",
    "    dataloaders = {}\n",
    "    \n",
    "    train_dataset = Detection_dataset(data_dir=data_dir, phase=\"train\", transform=transform)\n",
    "    dataloaders[\"train\"] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    valid_dataset = Detection_dataset(data_dir=data_dir, phase=\"valid\", transform=transform)    \n",
    "    dataloaders[\"val\"] = DataLoader(valid_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    return dataloaders\n",
    "\n",
    "dataloaders = train_valid_dataloader(data_dir, BATCH_SIZE, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b831911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "tensor([[[1, 2],\n",
      "         [5, 6]],\n",
      "\n",
      "        [[3, 4],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1,2],[3,4]])\n",
    "t2 = torch.tensor([[5,6],[7,8]])\n",
    "\n",
    "print(torch.cat((t1,t2),dim=0))\n",
    "print(torch.stack([t1,t2],dim=1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
