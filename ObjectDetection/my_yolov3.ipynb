{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde6c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "881d1075-432f-4150-9cdb-a8cbb28c634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "IMAGE_SIZE = 416\n",
    "BATCH_SIZE = 10\n",
    "data_dir = '../data/IndoorObjectsDetection'\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd66c9-bed4-49eb-b9bc-27d2ee786742",
   "metadata": {},
   "source": [
    "# **Data Load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "077e5acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'door',\n",
       " 1: 'cabinetDoor',\n",
       " 2: 'refrigeratorDoor',\n",
       " 3: 'window',\n",
       " 4: 'chair',\n",
       " 5: 'table',\n",
       " 6: 'cabinet',\n",
       " 7: 'couch',\n",
       " 8: 'openedDoor',\n",
       " 9: 'pole'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "import random\n",
    "\n",
    "data_config = open( data_dir + '/data.yaml')\n",
    "\n",
    "data_info = yaml.load(data_config, Loader=yaml.FullLoader)\n",
    "\n",
    "train_data_path = data_info['train_data_path']\n",
    "val_data_path = data_info['val_data_path']\n",
    "test_data_path = data_info['test_data_path']\n",
    "\n",
    "train_labels_path = data_info['train_labels_path']\n",
    "val_labels_path = data_info['val_labels_path']\n",
    "test_labels_path = data_info['test_labels_path']\n",
    "\n",
    "target_list = data_info['names']\n",
    "target_dict = dict(zip(range(len(target_list)), target_list))\n",
    "\n",
    "num_classes = len(target_list)\n",
    "\n",
    "target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a21b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 416, 416])\n",
      "torch.Size([1, 512, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained = False)\n",
    "layers = [m for m in resnet18.children()]\n",
    "\n",
    "# 마지막 2층인 average pooling & fully connected layer 은 back bone으로 사용하지 않음\n",
    "test_net = nn.Sequential(*layers[:-2]) \n",
    "\n",
    "temp_x = torch.randn(1,3,IMAGE_SIZE,IMAGE_SIZE)\n",
    "temp_y = test_net(temp_x)\n",
    "\n",
    "\n",
    "print(type(temp_x))\n",
    "print(temp_x.shape)\n",
    "print(temp_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dcc8e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass YOLOv1_RESNET(nn.Module):\\n    def __init__(self, num_classes):\\n        super().__init__()\\n        \\n        self.num_classes = num_classes\\n        self.num_bboxes = 2\\n        self.grid_size = 7\\n        \\n        resnet18 = torchvision.models.resnet18(pretrained = False)\\n        layers = [m for m in resnet18.children()]\\n        \\n        self.backbone = nn.Sequential(*layers[:-2])\\n        \\n        self.neck = nn.Sequential(\\n            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=1, padding=0, bias=False),\\n            nn.BatchNorm2d(1024),\\n            nn.ReLU(inplace=True),\\n            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1, bias=False),\\n            nn.BatchNorm2d(1024),\\n            nn.ReLU(inplace=True),\\n            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1, bias=False),\\n            nn.BatchNorm2d(1024),\\n            nn.ReLU(inplace=True)\\n        )\\n        \\n        self.head = nn.Sequential(\\n            nn.Conv2d(in_channels=1024, out_channels=5*self.num_bboxes+self.num_classes, kernel_size=1, padding=0, bias=False),\\n            nn.AdaptiveAvgPool2d(output_size=(self.grid_size, self.grid_size))\\n        )\\n        \\n    def forward(self, x):\\n        out = self.backbone(x)\\n        out = self.neck(out)\\n        out = self.head(out)\\n        return out\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class YOLOv1_RESNET(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_bboxes = 2\n",
    "        self.grid_size = 7\n",
    "        \n",
    "        resnet18 = torchvision.models.resnet18(pretrained = False)\n",
    "        layers = [m for m in resnet18.children()]\n",
    "        \n",
    "        self.backbone = nn.Sequential(*layers[:-2])\n",
    "        \n",
    "        self.neck = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1024, out_channels=5*self.num_bboxes+self.num_classes, kernel_size=1, padding=0, bias=False),\n",
    "            nn.AdaptiveAvgPool2d(output_size=(self.grid_size, self.grid_size))\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        out = self.neck(out)\n",
    "        out = self.head(out)\n",
    "        return out\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f925b4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNUM_CLASSES = len(target_list)\\nmodel = YOLOv1_RESNET(num_classes = NUM_CLASSES)\\n\\nmodel\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NUM_CLASSES = len(target_list)\n",
    "model = YOLOv1_RESNET(num_classes = NUM_CLASSES)\n",
    "\n",
    "model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f1e79dd-dd22-4d2e-956e-fca547da2e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHORS = [[[0.2788, 0.2163],\n",
    "         [0.3750, 0.4760],\n",
    "         [0.8966, 0.7837]],\n",
    "\n",
    "        [[0.0721, 0.1466],\n",
    "         [0.1490, 0.1082],\n",
    "         [0.1418, 0.2861]],\n",
    "\n",
    "        [[0.0240, 0.0312],\n",
    "         [0.0385, 0.0721],\n",
    "         [0.0793, 0.0553]]]\n",
    "\n",
    "GRID_SIZE = [13, 26, 52] \n",
    "\n",
    "scaled_anchors = torch.tensor(ANCHORS)*torch.tensor(GRID_SIZE).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02020a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detection_dataset():\n",
    "    def __init__(self, data_dir, phase, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.phase = phase\n",
    "        self.image_files = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        for fn in os.listdir(os.path.join(self.data_dir, phase, 'images')):\n",
    "            bboxes, class_ids = self.get_label(fn)\n",
    "                                \n",
    "            if(fn.endswith(\"jpg\") and bboxes.size != 0 and class_ids.size != 0):\n",
    "                self.image_files.append(fn)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename, image = self.get_image(index)\n",
    "        bboxes, class_ids = self.get_label(filename)\n",
    "        \n",
    "        if self.transform: \n",
    "            transformed_data = self.transform(image=image, bboxes=bboxes, class_ids=class_ids)\n",
    "            image = transformed_data['image']\n",
    "            bboxes = np.array(transformed_data['bboxes'])\n",
    "            class_ids = np.array(transformed_data['class_ids'])\n",
    "        else:\n",
    "            #transform 을 하지 않을경우 reshape to (C,W,H)\n",
    "            image = torch.Tensor(image).permute(2,0,1)\n",
    "        \n",
    "        target = np.concatenate((bboxes, class_ids[:, np.newaxis]), axis=1)\n",
    "\n",
    "        scaled_anchors_target = self.convert_target_to_scaled_anchors_target(target, ANCHORS)\n",
    "        \n",
    "        return image, scaled_anchors_target, target, filename\n",
    "    \n",
    "    def get_image(self, index):\n",
    "        filename = self.image_files[index]\n",
    "        image_path = os.path.join(self.data_dir, self.phase, 'images', filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        return filename, image\n",
    "    \n",
    "    def get_label(self, filename):\n",
    "        image_id = filename.split('.')[0]\n",
    "        label_file_path = os.path.join(self.data_dir, self.phase, 'labels') + '/' + image_id + '.txt'\n",
    "        try:\n",
    "            bbox_df = pd.read_csv(label_file_path, sep=' ', header=None)\n",
    "\n",
    "            # width or height 가 0이면 제거 \n",
    "            bbox_df = bbox_df[(bbox_df[3] != 0) & (bbox_df[4] != 0)]\n",
    "            \n",
    "            bboxes = np.asarray(bbox_df[[1,2,3,4]])\n",
    "            class_ids = np.asarray(bbox_df[0])\n",
    "            \n",
    "        except Exception as e:\n",
    "            bboxes = np.array([])\n",
    "            class_ids = np.array([])\n",
    "            \n",
    "        return bboxes, class_ids\n",
    "\n",
    "    def convert_target_to_scaled_anchors_target(self, bboxes, anchors):\n",
    "\n",
    "        self.grid_sizes = GRID_SIZE\n",
    "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2]) \n",
    "        # Number of anchor boxes  \n",
    "        self.num_anchors = self.anchors.shape[0] \n",
    "        # Number of anchor boxes per scale \n",
    "        self.num_anchors_per_scale = self.num_anchors // 3\n",
    "        # Ignore IoU threshold \n",
    "        self.ignore_iou_thresh = 0.5\n",
    "\n",
    "        \n",
    "        # Below assumes 3 scale predictions (as paper) and same num of anchors per scale \n",
    "        # target : [probabilities, x, y, width, height, class_label] \n",
    "        targets = [torch.zeros((self.num_anchors_per_scale, s, s, 6)) \n",
    "                   for s in self.grid_sizes] \n",
    "          \n",
    "        # Identify anchor box and cell for each bounding box \n",
    "        for box in bboxes: \n",
    "            # Calculate iou of bounding box with anchor boxes \n",
    "            iou_anchors = self.iou_WH(torch.tensor(box[2:4]), self.anchors)\n",
    "\n",
    "            # Selecting the best anchor box \n",
    "            anchor_indices = iou_anchors.argsort(descending=True, dim=0) \n",
    "            x, y, width, height, class_label = box \n",
    "  \n",
    "            # At each scale, assigning the bounding box to the  \n",
    "            # best matching anchor box \n",
    "            has_anchor = [False] * 3\n",
    "            for anchor_idx in anchor_indices: \n",
    "                scale_idx = anchor_idx // self.num_anchors_per_scale \n",
    "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale \n",
    "                  \n",
    "                # Identifying the grid size for the scale \n",
    "                s = self.grid_sizes[scale_idx] \n",
    "                  \n",
    "                # Identifying the cell to which the bounding box belongs \n",
    "                i, j = int(s * y), int(s * x) \n",
    "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0] \n",
    "                  \n",
    "                # Check if the anchor box is already assigned \n",
    "                if not anchor_taken and not has_anchor[scale_idx]: \n",
    "                    # Set the probability to 1 \n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
    "  \n",
    "                    # Calculating the center of the bounding box relative \n",
    "                    # to the cell \n",
    "                    x_cell, y_cell = s * x - j, s * y - i  \n",
    "  \n",
    "                    # Calculating the width and height of the bounding box  \n",
    "                    # relative to the cell \n",
    "                    width_cell, height_cell = (width * s, height * s) \n",
    "  \n",
    "                    # Idnetify the box coordinates \n",
    "                    box_coordinates = torch.tensor( \n",
    "                                        [x_cell, y_cell, width_cell,  \n",
    "                                         height_cell] \n",
    "                                    ) \n",
    "  \n",
    "                    # Assigning the box coordinates to the target \n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates \n",
    "  \n",
    "                    # Assigning the class label to the target \n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label) \n",
    "  \n",
    "                    # Set the anchor box as assigned for the scale \n",
    "                    has_anchor[scale_idx] = True\n",
    "  \n",
    "                # If the anchor box is already assigned, check if the  \n",
    "                # IoU is greater than the threshold \n",
    "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
    "                    # Set the probability to -1 to ignore the anchor box \n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1\n",
    "\n",
    "        return tuple(targets)\n",
    "\n",
    "    def iou_WH(self, box1, box2):\n",
    "        # IoU score based on width and height of bounding boxes \n",
    "          \n",
    "        # Calculate intersection area \n",
    "        intersection_area = torch.min(box1[..., 0], box2[..., 0]) * torch.min(box1[..., 1], box2[..., 1]) \n",
    "  \n",
    "        # Calculate union area \n",
    "        box1_area = box1[..., 0] * box1[..., 1] \n",
    "        box2_area = box2[..., 0] * box2[..., 1] \n",
    "        union_area = box1_area + box2_area - intersection_area \n",
    "  \n",
    "        # Calculate IoU score \n",
    "        iou_score = intersection_area / union_area \n",
    "  \n",
    "        # Return IoU score \n",
    "        return iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fcf18cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose([\n",
       "  Resize(p=1.0, height=416, width=416, interpolation=1),\n",
       "  ToTensorV2(p=1.0, transpose_mask=False),\n",
       "], p=1.0, bbox_params={'format': 'yolo', 'label_fields': ['class_ids'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True, 'clip': False}, keypoint_params=None, additional_targets={}, is_check_shapes=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\"\"\"\n",
    "    when you use yolo format bbox param\n",
    "    need to add logic in albumentations/augmentations/bbox_utils.py - check_bbox() method\n",
    "    to make bbox boundery in [0,1]\n",
    "    \n",
    "    -------------------\n",
    "    bbox=list(bbox)\n",
    "    \n",
    "    for i in range(4):\n",
    "      if (bbox[i]<0) :\n",
    "        bbox[i]=0\n",
    "      elif (bbox[i]>1) :\n",
    "        bbox[i]=1\n",
    "    \n",
    "    bbox=tuple(bbox)\n",
    "    --------------------\n",
    "\"\"\"\n",
    "\n",
    "#mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225) -> imageNet 데이터셋에 기반한 계산된 수치 \n",
    "transform = A.Compose([\n",
    "        A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "        #A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    ")\n",
    "\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec0c4207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd0da1fd75240fe93c6c9dde2cb12a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=860), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.utils import make_grid\n",
    "from my_util import set_bounding_boxes, set_bounding_box, get_random_color_dict\n",
    "from ipywidgets import interact\n",
    "\n",
    "transformed_train_dataset = Detection_dataset(data_dir=data_dir, phase=\"train\", transform=transform)\n",
    "\n",
    "@interact(index=(0, len(transformed_train_dataset)-1))\n",
    "def show_transformed_image(index=0):\n",
    "    img, scaled_anchors_target, target, filename = transformed_train_dataset[index]\n",
    "    \n",
    "    np_image = make_grid(img, normalize=False).permute(1,2,0).numpy()\n",
    "    res = set_bounding_boxes(np_image, target[:,0:4], 'yolo', target[:,4].astype(int), target_dict, get_random_color_dict(target_dict))\n",
    "\n",
    "    # np_image = make_grid(img, normalize=True).permute(1,2,0).numpy()\n",
    "    # np_image_unit8 = (np_image*255).astype(np.uint8)\n",
    "    # res = set_bounding_boxes(np_image_unit8, target[:,0:4], 'yolo', target[:,4].astype(int), target_dict, get_random_color_dict(target_dict))\n",
    "    \n",
    "    plt.imshow(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fe4aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image_list = []\n",
    "    scaled_anchors_target_list = []\n",
    "    target_list = []\n",
    "    filename_list = []\n",
    "    \n",
    "    for a,b,c,d in batch:\n",
    "        image_list.append(a)\n",
    "        scaled_anchors_target_list.append(b)\n",
    "        target_list.append(c)\n",
    "        filename_list.append(d)\n",
    "        \n",
    "    return torch.stack(image_list, dim=0), scaled_anchors_target_list, target_list, filename_list\n",
    "\n",
    "def train_valid_dataloader(data_dir, batch_size=4, transform=None):\n",
    "    dataloaders = {}\n",
    "    \n",
    "    train_dataset = Detection_dataset(data_dir=data_dir, phase=\"train\", transform=transform)\n",
    "    dataloaders[\"train\"] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    valid_dataset = Detection_dataset(data_dir=data_dir, phase=\"valid\", transform=transform)    \n",
    "    dataloaders[\"val\"] = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    return dataloaders\n",
    "\n",
    "dataloaders = train_valid_dataloader(data_dir, BATCH_SIZE, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "119930c0-3296-41da-a217-3d6fa5e4ee99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npip install --trusted-host pypi.python.org --trusted-host pypi.org --trusted-host files.pythonhosted.org tqdm\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pip install --trusted-host pypi.python.org --trusted-host pypi.org --trusted-host files.pythonhosted.org tqdm\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb81364-0e19-48b0-a177-41bdd67bad15",
   "metadata": {},
   "source": [
    "# **YOLO_V3 Model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e935a945-f31e-489f-9eac-8d3ee0cdbd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=(kernel_size-1)//2, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34287db5-5f0c-4869-aafd-fcefd826848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backbone\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            BasicConvBlock(channels, channels//2, kernel_size=1, stride=1),\n",
    "            BasicConvBlock(channels//2, channels, kernel_size=3, stride=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.residual(x) + x\n",
    "\n",
    "class DarkNet53(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = BasicConvBlock(3, 32, 3, 1)\n",
    "        self.block1 = nn.Sequential(\n",
    "            BasicConvBlock(32, 64, 3, 2),\n",
    "            ResidualBlock(64)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            BasicConvBlock(64, 128, 3, 2),\n",
    "            nn.Sequential(*[ResidualBlock(128) for _ in range(2)])\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            BasicConvBlock(128, 256, 3, 2),\n",
    "            nn.Sequential(*[ResidualBlock(256) for _ in range(8)])\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "            BasicConvBlock(256, 512, 3, 2),\n",
    "            nn.Sequential(*[ResidualBlock(512) for _ in range(8)])\n",
    "        )\n",
    "        self.block5 = nn.Sequential(\n",
    "            BasicConvBlock(512, 1024, 3, 2),\n",
    "            nn.Sequential(*[ResidualBlock(1024) for _ in range(4)])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        feature_map1 = self.block3(x)\n",
    "        feature_map2 = self.block4(feature_map1)\n",
    "        feature_map3 = self.block5(feature_map2)\n",
    "\n",
    "        return feature_map1, feature_map2, feature_map3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2d4016d-ce05-4a19-acac-55cf19ce00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neck : FPN top-down\n",
    "class FPN_featureBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            BasicConvBlock(in_channels, out_channels, 1),\n",
    "            BasicConvBlock(out_channels, out_channels*2, 3),\n",
    "            BasicConvBlock(out_channels*2, out_channels, 1),\n",
    "            BasicConvBlock(out_channels, out_channels*2, 3),\n",
    "            BasicConvBlock(out_channels*2, out_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "        \n",
    "class UpSampling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            BasicConvBlock(in_channels, out_channels, 1),\n",
    "            nn.Upsample(scale_factor = 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8bb601a-7291-4563-8f78-a816716d2a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Head\n",
    "class DetectionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, img_size=IMAGE_SIZE):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = 3\n",
    "        \n",
    "        #(4+1+num_classes)*3 -> [4(x,y,w,h) + 1(Objectness Score) + num_classes(Class Probabilities)] * 3(number of anchors)\n",
    "        self.pred = nn.Sequential(\n",
    "            BasicConvBlock(in_channels, in_channels*2, 3),\n",
    "            nn.Conv2d(in_channels*2, (4+1+num_classes)*3, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        grid_size = x.size(2)\n",
    "        \n",
    "        output = self.pred(x)\n",
    "        output = output.view(batch_size, self.num_anchors, 4+1+self.num_classes, grid_size, grid_size) \n",
    "        output = output.permute(0, 1, 3, 4, 2)\n",
    "        \n",
    "        output = output.contiguous()\n",
    "\n",
    "        '''\n",
    "        obj_score = torch.sigmoid(output[..., 4]) # 4+1+self.num_classes 에서 '1'의 Confidence: 1 if object, else 0\n",
    "        pred_cls = torch.sigmoid(output[..., 5:]) # 4+1+self.num_classes 에서 'self.num_classes' 의 class classify\n",
    "\n",
    "        # grid_size 갱신\n",
    "        if grid_size != self.grid_size:\n",
    "            # grid_size를 갱신하고, transform_outputs 함수를 위해 anchor 박스를 전처리 합니다.\n",
    "            self.compute_grid_offsets(grid_size)\n",
    "\n",
    "        # calculate bounding box coordinates\n",
    "        pred_boxes = self.transform_outputs(output)\n",
    "\n",
    "        # output shape(batch, num_anchors x S x S, 25)\n",
    "        # ex) at 13x13 -> [batch, 507(3x13x13), 25], at 26x26 -> [batch, 2028, 25], at 52x52 -> [batch, 8112, 25]\n",
    "        # 최종적으로 YOLO는 10647개의 바운딩박스를 예측합니다.\n",
    "        output = torch.cat((pred_boxes.view(batch_size, -1, 4),\n",
    "                    obj_score.view(batch_size, -1, 1),\n",
    "                    pred_cls.view(batch_size, -1, self.num_classes)), -1)\n",
    "\n",
    "        '''\n",
    "        \n",
    "        return output\n",
    "\n",
    "    '''\n",
    "    # grid_size를 갱신하고, transform_outputs 함수를 위해 anchor 박스를 전처리\n",
    "    def compute_grid_offsets(self, grid_size):\n",
    "        self.grid_size = grid_size # ex) 13, 26, 52\n",
    "        self.stride = self.img_size / self.grid_size\n",
    "\n",
    "        # cell index 생성\n",
    "        # transform_outputs 함수에서 바운딩 박스의 x, y좌표를 예측할 때 사용\n",
    "        self.grid_x = torch.arange(grid_size, device=device).repeat(1, 1, grid_size, 1).type(torch.float32)                # 1, 1, S, S\n",
    "        self.grid_y = torch.arange(grid_size, device=device).repeat(1, 1, grid_size, 1).transpose(3,2).type(torch.float32) # 1, 1, S, S\n",
    "\n",
    "        # transform_outputs 함수에서 바운딩 박스의 w, h를 예측할 때 사용\n",
    "        # shape=(3,2) -> (1, 3, 1, 1)\n",
    "        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, len(self.scaled_anchors), 1, 1))\n",
    "        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, len(self.scaled_anchors), 1, 1))\n",
    "\n",
    "    # 예측한 바운딩 박스 좌표를 계산하는 함수입니다.\n",
    "    def transform_outputs(self, prediction):\n",
    "        # prediction = (batch, num_anchors, S, S, coordinates + Objectness Score + classes)\n",
    "        x = prediction[..., 0] # 1, 3, S, S\n",
    "        y = prediction[..., 1] # 1, 3, S, S\n",
    "        w = prediction[..., 2] # 예측한 바운딩 박스 너비\n",
    "        h = prediction[..., 3] # 예측한 바운딩 박스 높이\n",
    "        \n",
    "        pred_boxes = torch.zeros_like(prediction[..., :4]).to(device)\n",
    "        pred_boxes[..., 0] = torch.sigmoid(x) + self.grid_x # sigmoid(box x) + cell x 좌표\n",
    "        pred_boxes[..., 1] = torch.sigmoid(y) + self.grid_y # sigmoid(box y) + cell y 좌표\n",
    "        pred_boxes[..., 2] = torch.exp(w) * self.anchor_w\n",
    "        pred_boxes[..., 3] = torch.exp(h) * self.anchor_h\n",
    "\n",
    "        return pred_boxes * self.stride\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "febfb77d-ab9d-4b82-91fe-7d4565ab6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolov3(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.darknet53 = DarkNet53()\n",
    "\n",
    "        self.fpn_feature_block1 = FPN_featureBlock(1024, 512)\n",
    "        self.detectionlayer1 = DetectionLayer(512, num_classes)\n",
    "        self.upsampling1 = UpSampling(512, 256)\n",
    "\n",
    "        self.fpn_feature_block2 = FPN_featureBlock(512+256, 256)\n",
    "        self.detectionlayer2 = DetectionLayer(256, num_classes)\n",
    "        self.upsampling2 = UpSampling(256, 128)\n",
    "        \n",
    "        self.fpn_feature_block3 = FPN_featureBlock(256+128, 128)\n",
    "        self.detectionlayer3 = DetectionLayer(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.feature1, self.feature2, self.feature3 = self.darknet53(x)\n",
    "        \n",
    "        x = self.fpn_feature_block1(self.feature3)\n",
    "        output1 = self.detectionlayer1(x)\n",
    "        x = self.upsampling1(x)\n",
    "\n",
    "        x = self.fpn_feature_block2(torch.cat([x, self.feature2], dim=1))\n",
    "        output2 = self.detectionlayer2(x)\n",
    "        x = self.upsampling2(x)\n",
    "\n",
    "        x = self.fpn_feature_block3(torch.cat([x, self.feature1], dim=1))\n",
    "        output3 = self.detectionlayer3(x)\n",
    "\n",
    "        return output1, output2, output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f57f988f-b3ea-42f0-85b6-5c5d7b5a0bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx = torch.randint(0, 255, (1, 3, 416, 416)).float()\\nout = model(x)\\nprint(out[0].shape) # torch.Size([1, 3, 13, 13, 15]) -> 3*13*13 = 507 개의 bounding box 예측\\nprint(out[1].shape) # torch.Size([1, 3, 26, 26, 15]) -> 3*26*26 = 2028 개의 bounding box 예측\\nprint(out[2].shape) # torch.Size([1, 3, 52, 52, 25]) -> 3*52*52 = 8112 개의 bounding box 예측\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Yolov3(num_classes).to(device)\n",
    "\n",
    "'''\n",
    "x = torch.randint(0, 255, (1, 3, 416, 416)).float()\n",
    "out = model(x)\n",
    "print(out[0].shape) # torch.Size([1, 3, 13, 13, 15]) -> 3*13*13 = 507 개의 bounding box 예측\n",
    "print(out[1].shape) # torch.Size([1, 3, 26, 26, 15]) -> 3*26*26 = 2028 개의 bounding box 예측\n",
    "print(out[2].shape) # torch.Size([1, 3, 52, 52, 25]) -> 3*52*52 = 8112 개의 bounding box 예측\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5ca1fd9-9743-4121-975d-9fe613067b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "Yolov3                                                       [1, 3, 13, 13, 15]        --\n",
       "├─DarkNet53: 1-1                                             [1, 256, 52, 52]          --\n",
       "│    └─BasicConvBlock: 2-1                                   [1, 32, 416, 416]         --\n",
       "│    │    └─Sequential: 3-1                                  [1, 32, 416, 416]         928\n",
       "│    └─Sequential: 2-2                                       [1, 64, 208, 208]         --\n",
       "│    │    └─BasicConvBlock: 3-2                              [1, 64, 208, 208]         18,560\n",
       "│    │    └─ResidualBlock: 3-3                               [1, 64, 208, 208]         20,672\n",
       "│    └─Sequential: 2-3                                       [1, 128, 104, 104]        --\n",
       "│    │    └─BasicConvBlock: 3-4                              [1, 128, 104, 104]        73,984\n",
       "│    │    └─Sequential: 3-5                                  [1, 128, 104, 104]        164,608\n",
       "│    └─Sequential: 2-4                                       [1, 256, 52, 52]          --\n",
       "│    │    └─BasicConvBlock: 3-6                              [1, 256, 52, 52]          295,424\n",
       "│    │    └─Sequential: 3-7                                  [1, 256, 52, 52]          2,627,584\n",
       "│    └─Sequential: 2-5                                       [1, 512, 26, 26]          --\n",
       "│    │    └─BasicConvBlock: 3-8                              [1, 512, 26, 26]          1,180,672\n",
       "│    │    └─Sequential: 3-9                                  [1, 512, 26, 26]          10,498,048\n",
       "│    └─Sequential: 2-6                                       [1, 1024, 13, 13]         --\n",
       "│    │    └─BasicConvBlock: 3-10                             [1, 1024, 13, 13]         4,720,640\n",
       "│    │    └─Sequential: 3-11                                 [1, 1024, 13, 13]         20,983,808\n",
       "├─FPN_featureBlock: 1-2                                      [1, 512, 13, 13]          --\n",
       "│    └─Sequential: 2-7                                       [1, 512, 13, 13]          --\n",
       "│    │    └─BasicConvBlock: 3-12                             [1, 512, 13, 13]          525,312\n",
       "│    │    └─BasicConvBlock: 3-13                             [1, 1024, 13, 13]         4,720,640\n",
       "│    │    └─BasicConvBlock: 3-14                             [1, 512, 13, 13]          525,312\n",
       "│    │    └─BasicConvBlock: 3-15                             [1, 1024, 13, 13]         4,720,640\n",
       "│    │    └─BasicConvBlock: 3-16                             [1, 512, 13, 13]          525,312\n",
       "├─DetectionLayer: 1-3                                        [1, 3, 13, 13, 15]        --\n",
       "│    └─Sequential: 2-8                                       [1, 45, 13, 13]           --\n",
       "│    │    └─BasicConvBlock: 3-17                             [1, 1024, 13, 13]         4,720,640\n",
       "│    │    └─Conv2d: 3-18                                     [1, 45, 13, 13]           46,125\n",
       "├─UpSampling: 1-4                                            [1, 256, 26, 26]          --\n",
       "│    └─Sequential: 2-9                                       [1, 256, 26, 26]          --\n",
       "│    │    └─BasicConvBlock: 3-19                             [1, 256, 13, 13]          131,584\n",
       "│    │    └─Upsample: 3-20                                   [1, 256, 26, 26]          --\n",
       "├─FPN_featureBlock: 1-5                                      [1, 256, 26, 26]          --\n",
       "│    └─Sequential: 2-10                                      [1, 256, 26, 26]          --\n",
       "│    │    └─BasicConvBlock: 3-21                             [1, 256, 26, 26]          197,120\n",
       "│    │    └─BasicConvBlock: 3-22                             [1, 512, 26, 26]          1,180,672\n",
       "│    │    └─BasicConvBlock: 3-23                             [1, 256, 26, 26]          131,584\n",
       "│    │    └─BasicConvBlock: 3-24                             [1, 512, 26, 26]          1,180,672\n",
       "│    │    └─BasicConvBlock: 3-25                             [1, 256, 26, 26]          131,584\n",
       "├─DetectionLayer: 1-6                                        [1, 3, 26, 26, 15]        --\n",
       "│    └─Sequential: 2-11                                      [1, 45, 26, 26]           --\n",
       "│    │    └─BasicConvBlock: 3-26                             [1, 512, 26, 26]          1,180,672\n",
       "│    │    └─Conv2d: 3-27                                     [1, 45, 26, 26]           23,085\n",
       "├─UpSampling: 1-7                                            [1, 128, 52, 52]          --\n",
       "│    └─Sequential: 2-12                                      [1, 128, 52, 52]          --\n",
       "│    │    └─BasicConvBlock: 3-28                             [1, 128, 26, 26]          33,024\n",
       "│    │    └─Upsample: 3-29                                   [1, 128, 52, 52]          --\n",
       "├─FPN_featureBlock: 1-8                                      [1, 128, 52, 52]          --\n",
       "│    └─Sequential: 2-13                                      [1, 128, 52, 52]          --\n",
       "│    │    └─BasicConvBlock: 3-30                             [1, 128, 52, 52]          49,408\n",
       "│    │    └─BasicConvBlock: 3-31                             [1, 256, 52, 52]          295,424\n",
       "│    │    └─BasicConvBlock: 3-32                             [1, 128, 52, 52]          33,024\n",
       "│    │    └─BasicConvBlock: 3-33                             [1, 256, 52, 52]          295,424\n",
       "│    │    └─BasicConvBlock: 3-34                             [1, 128, 52, 52]          33,024\n",
       "├─DetectionLayer: 1-9                                        [1, 3, 52, 52, 15]        --\n",
       "│    └─Sequential: 2-14                                      [1, 45, 52, 52]           --\n",
       "│    │    └─BasicConvBlock: 3-35                             [1, 256, 52, 52]          295,424\n",
       "│    │    └─Conv2d: 3-36                                     [1, 45, 52, 52]           11,565\n",
       "==============================================================================================================\n",
       "Total params: 61,572,199\n",
       "Trainable params: 61,572,199\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 32.68\n",
       "==============================================================================================================\n",
       "Input size (MB): 2.08\n",
       "Forward/backward pass size (MB): 613.90\n",
       "Params size (MB): 246.29\n",
       "Estimated Total Size (MB): 862.26\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size = (1, 3, 416, 416), device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a9e318-6f8c-4a08-bd36-be4ca323133c",
   "metadata": {},
   "source": [
    "# **YOLO_V3 Loss** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f513a81f-746d-4919-8f90-e62de2f7eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining YOLO loss class \n",
    "class YOLOLoss(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__() \n",
    "        self.mse = nn.MSELoss() \n",
    "        self.bce = nn.BCEWithLogitsLoss() \n",
    "        self.cross_entropy = nn.CrossEntropyLoss() \n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "\n",
    "    # Defining a function to calculate Intersection over Union (IoU) \n",
    "    def iou(box1, box2): \n",
    "        # IoU score for prediction and label \n",
    "        # box1 (prediction) and box2 (label) are both in [x, y, width, height] format \n",
    "          \n",
    "        # Box coordinates of prediction \n",
    "        b1_x1 = box1[..., 0:1] - box1[..., 2:3] / 2\n",
    "        b1_y1 = box1[..., 1:2] - box1[..., 3:4] / 2\n",
    "        b1_x2 = box1[..., 0:1] + box1[..., 2:3] / 2\n",
    "        b1_y2 = box1[..., 1:2] + box1[..., 3:4] / 2\n",
    "  \n",
    "        # Box coordinates of ground truth \n",
    "        b2_x1 = box2[..., 0:1] - box2[..., 2:3] / 2\n",
    "        b2_y1 = box2[..., 1:2] - box2[..., 3:4] / 2\n",
    "        b2_x2 = box2[..., 0:1] + box2[..., 2:3] / 2\n",
    "        b2_y2 = box2[..., 1:2] + box2[..., 3:4] / 2\n",
    "  \n",
    "        # Get the coordinates of the intersection rectangle \n",
    "        x1 = torch.max(b1_x1, b2_x1) \n",
    "        y1 = torch.max(b1_y1, b2_y1) \n",
    "        x2 = torch.min(b1_x2, b2_x2) \n",
    "        y2 = torch.min(b1_y2, b2_y2) \n",
    "        # Make sure the intersection is at least 0 \n",
    "        intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0) \n",
    "  \n",
    "        # Calculate the union area \n",
    "        box1_area = abs((b1_x2 - b1_x1) * (b1_y2 - b1_y1)) \n",
    "        box2_area = abs((b2_x2 - b2_x1) * (b2_y2 - b2_y1)) \n",
    "        union = box1_area + box2_area - intersection \n",
    "  \n",
    "        # Calculate the IoU score \n",
    "        epsilon = 1e-6\n",
    "        iou_score = intersection / (union + epsilon) \n",
    "  \n",
    "        # Return IoU score \n",
    "        return iou_score \n",
    "      \n",
    "    def forward(self, pred, target, anchors): \n",
    "        # Identifying which cells in target have objects  \n",
    "        # and which have no objects \n",
    "        obj = target[..., 0] == 1\n",
    "        no_obj = target[..., 0] == 0\n",
    "  \n",
    "        # Calculating No object loss \n",
    "        no_object_loss = self.bce( \n",
    "            (pred[..., 0:1][no_obj]), (target[..., 0:1][no_obj]), \n",
    "        ) \n",
    "  \n",
    "          \n",
    "        # Reshaping anchors to match predictions \n",
    "        anchors = anchors.reshape(1, 3, 1, 1, 2) \n",
    "        # Box prediction confidence \n",
    "        box_preds = torch.cat([self.sigmoid(pred[..., 1:3]), \n",
    "                               torch.exp(pred[..., 3:5]) * anchors \n",
    "                            ],dim=-1) \n",
    "        # Calculating intersection over union for prediction and target \n",
    "        ious = self.iou(box_preds[obj], target[..., 1:5][obj]).detach() \n",
    "        # Calculating Object loss \n",
    "        object_loss = self.mse(self.sigmoid(pred[..., 0:1][obj]), \n",
    "                               ious * target[..., 0:1][obj]) \n",
    "  \n",
    "          \n",
    "        # Predicted box coordinates \n",
    "        pred[..., 1:3] = self.sigmoid(pred[..., 1:3]) \n",
    "        # Target box coordinates \n",
    "        target[..., 3:5] = torch.log(1e-6 + target[..., 3:5] / anchors) \n",
    "        # Calculating box coordinate loss \n",
    "        box_loss = self.mse(pred[..., 1:5][obj], \n",
    "                            target[..., 1:5][obj]) \n",
    "  \n",
    "          \n",
    "        # Claculating class loss \n",
    "        class_loss = self.cross_entropy((pred[..., 5:][obj]), \n",
    "                                   target[..., 5][obj].long()) \n",
    "  \n",
    "        # Total loss \n",
    "        return ( \n",
    "            box_loss \n",
    "            + object_loss \n",
    "            + no_object_loss \n",
    "            + class_loss \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa26fcf-f699-453f-87db-c5b20dbfbb30",
   "metadata": {},
   "source": [
    "# **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb1bcad4-e874-4a32-b96f-832cf0f6fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Define the train function to train the model \n",
    "def training_loop(loader, model, optimizer, loss_fn, scaler, scaled_anchors): \n",
    "    # Creating a progress bar \n",
    "    progress_bar = tqdm(loader[\"train\"], leave=True) \n",
    "  \n",
    "    # Initializing a list to store the losses \n",
    "    train_losses = [] \n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    # Iterating over the training data \n",
    "    for _, (tx, tensor_ty, _, _) in enumerate(progress_bar): \n",
    "        tx = tx.to(device)\n",
    "        ty = torch.stack([t.item() for t in tensor_ty[0]])\n",
    "        \n",
    "        ty0, ty1, ty2 = ( \n",
    "            ty[0].to(device), \n",
    "            ty[1].to(device), \n",
    "            ty[2].to(device), \n",
    "        ) \n",
    "  \n",
    "        with torch.cuda.amp.autocast(): \n",
    "            # Getting the model predictions \n",
    "            train_outputs = model(tx) \n",
    "            # Calculating the loss at each scale \n",
    "            train_loss = ( \n",
    "                  loss_fn(train_outputs[0], ty0, scaled_anchors[0]) \n",
    "                + loss_fn(train_outputs[1], ty1, scaled_anchors[1]) \n",
    "                + loss_fn(train_outputs[2], ty2, scaled_anchors[2]) \n",
    "            ) \n",
    "  \n",
    "        # Add the loss to the list \n",
    "        train_losses.append(train_loss.item()) \n",
    "  \n",
    "        # Reset gradients \n",
    "        optimizer.zero_grad() \n",
    "  \n",
    "        # Backpropagate the loss \n",
    "        scaler.scale(train_loss).backward() \n",
    "  \n",
    "        # Optimization step \n",
    "        scaler.step(optimizer) \n",
    "  \n",
    "        # Update the scaler for next iteration \n",
    "        scaler.update() \n",
    "  \n",
    "        # update progress bar with loss \n",
    "        mean_train_loss = sum(train_losses) / len(train_losses) \n",
    "        progress_bar.set_postfix(train_loss=mean_train_loss)\n",
    "\n",
    "    val_losses = [] \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (vx, tensor_vy, _, _) in loader[\"val\"]:\n",
    "            vx = vx.to(device)\n",
    "            vy = torch.stack([v.item() for v in tensor_vy[0]])\n",
    "    \n",
    "            vy0, vy1, vy2 = ( \n",
    "                vy[0].to(device), \n",
    "                vy[1].to(device), \n",
    "                vy[2].to(device), \n",
    "            ) \n",
    "      \n",
    "            with torch.cuda.amp.autocast(): \n",
    "                # Getting the model predictions \n",
    "                val_outputs = model(vx) \n",
    "                # Calculating the loss at each scale \n",
    "                val_loss = ( \n",
    "                      loss_fn(val_outputs[0], vy0, scaled_anchors[0]) \n",
    "                    + loss_fn(val_outputs[1], vy1, scaled_anchors[1]) \n",
    "                    + loss_fn(val_outputs[2], vy2, scaled_anchors[2]) \n",
    "                ) \n",
    "      \n",
    "            # Add the loss to the list \n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            # update progress bar with loss \n",
    "            mean_val_loss = sum(val_losses) / len(val_losses)\n",
    "            progress_bar.set_postfix(val_loss=mean_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0578cc5-324d-4ab2-8e23-be29b6a89af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/87 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 3042 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m): \n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e) \n\u001b[1;32m---> 16\u001b[0m     \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaled_anchors\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Saving the model \u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_model: \n",
      "Cell \u001b[1;32mIn[30], line 16\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(loader, model, optimizer, loss_fn, scaler, scaled_anchors)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, (tx, tensor_ty, _, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress_bar): \n\u001b[0;32m     15\u001b[0m     tx \u001b[38;5;241m=\u001b[39m tx\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 16\u001b[0m     ty \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensor_ty[\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m     18\u001b[0m     ty0, ty1, ty2 \u001b[38;5;241m=\u001b[39m ( \n\u001b[0;32m     19\u001b[0m         ty[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[0;32m     20\u001b[0m         ty[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[0;32m     21\u001b[0m         ty[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[0;32m     22\u001b[0m     ) \n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(): \n\u001b[0;32m     25\u001b[0m         \u001b[38;5;66;03m# Getting the model predictions \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, (tx, tensor_ty, _, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress_bar): \n\u001b[0;32m     15\u001b[0m     tx \u001b[38;5;241m=\u001b[39m tx\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 16\u001b[0m     ty \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensor_ty[\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m     18\u001b[0m     ty0, ty1, ty2 \u001b[38;5;241m=\u001b[39m ( \n\u001b[0;32m     19\u001b[0m         ty[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[0;32m     20\u001b[0m         ty[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[0;32m     21\u001b[0m         ty[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[0;32m     22\u001b[0m     ) \n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(): \n\u001b[0;32m     25\u001b[0m         \u001b[38;5;66;03m# Getting the model predictions \u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a Tensor with 3042 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim \n",
    "\n",
    "# Defining the optimizer \n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate) \n",
    "  \n",
    "# Defining the loss function \n",
    "loss_fn = YOLOLoss() \n",
    "  \n",
    "# Defining the scaler for mixed precision training \n",
    "scaler = torch.cuda.amp.GradScaler() \n",
    "\n",
    "epochs = 1\n",
    "\n",
    "for e in range(1, epochs+1): \n",
    "    print(\"Epoch:\", e) \n",
    "    training_loop(dataloaders, model, optimizer, loss_fn, scaler, scaled_anchors) \n",
    "  \n",
    "    # Saving the model \n",
    "    if save_model: \n",
    "        save_checkpoint(model, optimizer, filename=f\"checkpoint.pth.tar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
