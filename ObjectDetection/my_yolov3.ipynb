{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde6c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077e5acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'door',\n",
       " 1: 'cabinetDoor',\n",
       " 2: 'refrigeratorDoor',\n",
       " 3: 'window',\n",
       " 4: 'chair',\n",
       " 5: 'table',\n",
       " 6: 'cabinet',\n",
       " 7: 'couch',\n",
       " 8: 'openedDoor',\n",
       " 9: 'pole'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "import random\n",
    "\n",
    "data_config = open('../data/IndoorObjectsDetection/data.yaml')\n",
    "\n",
    "data_info = yaml.load(data_config, Loader=yaml.FullLoader)\n",
    "\n",
    "train_data_path = data_info['train_data_path']\n",
    "val_data_path = data_info['val_data_path']\n",
    "test_data_path = data_info['test_data_path']\n",
    "\n",
    "train_labels_path = data_info['train_labels_path']\n",
    "val_labels_path = data_info['val_labels_path']\n",
    "test_labels_path = data_info['test_labels_path']\n",
    "\n",
    "target_list = data_info['names']\n",
    "target_dict = dict(zip(range(len(target_list)), target_list))\n",
    "\n",
    "target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a21b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 224\n",
    "\n",
    "resnet18 = torchvision.models.resnet18(pretrained = False)\n",
    "layers = [m for m in resnet18.children()]\n",
    "\n",
    "# 마지막 2층인 average pooling & fully connected layer 은 back bone으로 사용하지 않음\n",
    "test_net = nn.Sequential(*layers[:-2]) \n",
    "\n",
    "temp_x = torch.randn(1,3,IMAGE_SIZE,IMAGE_SIZE)\n",
    "temp_y = test_net(temp_x)\n",
    "\n",
    "\n",
    "print(type(temp_x))\n",
    "print(temp_x.shape)\n",
    "print(temp_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dcc8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv1_RESNET(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_bboxes = 2\n",
    "        self.grid_size = 7\n",
    "        \n",
    "        resnet18 = torchvision.models.resnet18(pretrained = False)\n",
    "        layers = [m for m in resnet18.children()]\n",
    "        \n",
    "        self.backbone = nn.Sequential(*layers[:-2])\n",
    "        \n",
    "        self.neck = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1024, out_channels=5*self.num_bboxes+self.num_classes, kernel_size=1, padding=0, bias=False),\n",
    "            nn.AdaptiveAvgPool2d(output_size=(self.grid_size, self.grid_size))\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        out = self.neck(out)\n",
    "        out = self.head(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f925b4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLOv1_RESNET(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (neck): Sequential(\n",
       "    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): Conv2d(1024, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = len(target_list)\n",
    "model = YOLOv1_RESNET(num_classes = NUM_CLASSES)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02020a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detection_dataset():\n",
    "    def __init__(self, data_dir, phase, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.phase = phase\n",
    "        self.image_files = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        for fn in os.listdir(os.path.join(self.data_dir, phase, 'images')):\n",
    "            bboxes, class_ids = self.get_label(fn)\n",
    "                                \n",
    "            if(fn.endswith(\"jpg\") and bboxes.size != 0 and class_ids.size != 0):\n",
    "                self.image_files.append(fn)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename, image = self.get_image(index)\n",
    "        bboxes, class_ids = self.get_label(filename)\n",
    "        \n",
    "        if self.transform: \n",
    "            transformed_data = self.transform(image=image, bboxes=bboxes, class_ids=class_ids)\n",
    "            image = transformed_data['image']\n",
    "            bboxes = np.array(transformed_data['bboxes'])\n",
    "            class_ids = np.array(transformed_data['class_ids'])\n",
    "        else:\n",
    "            #transform 을 하지 않을경우 reshape to (C,W,H)\n",
    "            image = torch.Tensor(image).permute(2,0,1)\n",
    "        \n",
    "        target = np.concatenate((bboxes, class_ids[:, np.newaxis]), axis=1)\n",
    "        return image, target, filename\n",
    "    \n",
    "    def get_image(self, index):\n",
    "        filename = self.image_files[index]\n",
    "        image_path = os.path.join(self.data_dir, self.phase, 'images', filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        return filename, image\n",
    "    \n",
    "    \n",
    "    def get_label(self, filename):\n",
    "        image_id = filename.split('.')[0]\n",
    "        label_file_path = os.path.join(self.data_dir, self.phase, 'labels') + '/' + image_id + '.txt'\n",
    "        try:\n",
    "            bbox_df = pd.read_csv(label_file_path, sep=' ', header=None)\n",
    "            \n",
    "            bboxes = np.asarray(bbox_df[[1,2,3,4]])\n",
    "            class_ids = np.asarray(bbox_df[0])\n",
    "            \n",
    "        except Exception as e:\n",
    "            bboxes = np.array([])\n",
    "            class_ids = np.array([])\n",
    "            \n",
    "            \n",
    "        return bboxes, class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fcf18cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose([\n",
       "  Resize(p=1.0, height=448, width=448, interpolation=1),\n",
       "  Normalize(p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, normalization='standard'),\n",
       "  ToTensorV2(p=1.0, transpose_mask=False),\n",
       "], p=1.0, bbox_params={'format': 'yolo', 'label_fields': ['class_ids'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True, 'clip': False}, keypoint_params=None, additional_targets={}, is_check_shapes=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "IMAGE_SIZE = 448\n",
    "\n",
    "\"\"\"\n",
    "    when you use yolo format bbox param\n",
    "    need to add logic in albumentations/augmentations/bbox_utils.py - check_bbox() method\n",
    "    to make bbox boundery in [0,1]\n",
    "    \n",
    "    -------------------\n",
    "    bbox=list(bbox)\n",
    "    \n",
    "    for i in range(4):\n",
    "      if (bbox[i]<0) :\n",
    "        bbox[i]=0\n",
    "      elif (bbox[i]>1) :\n",
    "        bbox[i]=1\n",
    "    \n",
    "    bbox=tuple(bbox)\n",
    "    --------------------\n",
    "\"\"\"\n",
    "\n",
    "#mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225) -> imageNet 데이터셋에 기반한 계산된 수치 \n",
    "transform = A.Compose([\n",
    "        A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    ")\n",
    "\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec0c4207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb06b295d1b4448bb4ea5795cd1c57c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=861), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.utils import make_grid\n",
    "from my_util import set_bounding_boxes, set_bounding_box, get_random_color_dict\n",
    "from ipywidgets import interact\n",
    "\n",
    "transformed_train_dataset = Detection_dataset(data_dir='../data/IndoorObjectsDetection', phase=\"train\", transform=transform)\n",
    "\n",
    "@interact(index=(0, len(transformed_train_dataset)-1))\n",
    "def show_transformed_image(index=0):\n",
    "    img, target, filename = transformed_train_dataset[index]\n",
    "    \n",
    "    np_image = make_grid(img, normalize=True).permute(1,2,0).numpy()\n",
    "    np_image_unit8 = (np_image*255).astype(np.uint8)\n",
    "    \n",
    "    res = set_bounding_boxes(np_image_unit8, target[:,0:4], 'yolo', target[:,4].astype(int), target_dict, get_random_color_dict(target_dict))\n",
    "    plt.imshow(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fe4aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "data_dir = '../data/IndoorObjectsDetection'\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image_list = []\n",
    "    target_list = []\n",
    "    filename_list = []\n",
    "    \n",
    "    for a,b,c in batch:\n",
    "        image_list.append(a)\n",
    "        target_list.append(b)\n",
    "        filename_list.append(c)\n",
    "        \n",
    "    return torch.stack(image_list, dim=0), target_list, filename_list\n",
    "\n",
    "def train_valid_dataloader(data_dir, batch_size=4, transform=None):\n",
    "    dataloaders = {}\n",
    "    \n",
    "    train_dataset = Detection_dataset(data_dir=data_dir, phase=\"train\", transform=transform)\n",
    "    dataloaders[\"train\"] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    valid_dataset = Detection_dataset(data_dir=data_dir, phase=\"valid\", transform=transform)    \n",
    "    dataloaders[\"val\"] = DataLoader(valid_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    return dataloaders\n",
    "\n",
    "dataloaders = train_valid_dataloader(data_dir, BATCH_SIZE, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b831911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "tensor([[[1, 2],\n",
      "         [5, 6]],\n",
      "\n",
      "        [[3, 4],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1,2],[3,4]])\n",
    "t2 = torch.tensor([[5,6],[7,8]])\n",
    "\n",
    "print(torch.cat((t1,t2),dim=0))\n",
    "print(torch.cat((t1,t2),dim=1))\n",
    "\n",
    "print(torch.stack([t1,t2],dim=0))\n",
    "print(torch.stack([t1,t2],dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "119930c0-3296-41da-a217-3d6fa5e4ee99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npip install --trusted-host pypi.python.org --trusted-host pypi.org --trusted-host files.pythonhosted.org torchinfo\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pip install --trusted-host pypi.python.org --trusted-host pypi.org --trusted-host files.pythonhosted.org torchinfo\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e935a945-f31e-489f-9eac-8d3ee0cdbd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=(kernel_size-1)//2, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34287db5-5f0c-4869-aafd-fcefd826848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backbone\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            BasicConvBlock(channels, channels//2, kernel_size=1, stride=1),\n",
    "            BasicConvBlock(channels//2, channels, kernel_size=3, stride=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.residual(x) + x\n",
    "\n",
    "class DarkNet53(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = BasicConvBlock(3, 32, 3, 1)\n",
    "        self.block1 = nn.Sequential(\n",
    "            BasicConvBlock(32, 64, 3, 2),\n",
    "            ResidualBlock(64)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            BasicConvBlock(64, 128, 3, 2),\n",
    "            nn.Sequential(*[ResidualBlock(128) for _ in range(2)])\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            BasicConvBlock(128, 256, 3, 2),\n",
    "            nn.Sequential(*[ResidualBlock(256) for _ in range(8)])\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "            BasicConvBlock(256, 512, 3, 2),\n",
    "            nn.Sequential(*[ResidualBlock(512) for _ in range(8)])\n",
    "        )\n",
    "        self.block5 = nn.Sequential(\n",
    "            BasicConvBlock(512, 1024, 3, 2),\n",
    "            nn.Sequential(*[ResidualBlock(1024) for _ in range(4)])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        feature_map1 = self.block3(x)\n",
    "        feature_map2 = self.block4(feature_map1)\n",
    "        feature_map3 = self.block5(feature_map2)\n",
    "\n",
    "        return feature_map1, feature_map2, feature_map3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2d4016d-ce05-4a19-acac-55cf19ce00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neck : FPN top-down\n",
    "class FPN_featureBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            BasicConvBlock(in_channels, out_channels, 1),\n",
    "            BasicConvBlock(out_channels, out_channels*2, 3),\n",
    "            BasicConvBlock(out_channels*2, out_channels, 1),\n",
    "            BasicConvBlock(out_channels, out_channels*2, 3),\n",
    "            BasicConvBlock(out_channels*2, out_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "        \n",
    "class UpSampling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            BasicConvBlock(in_channels, out_channels, 1),\n",
    "            nn.Upsample(scale_factor = 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8bb601a-7291-4563-8f78-a816716d2a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Head\n",
    "class DetectionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pred = nn.Sequential(\n",
    "            BasicConvBlock(in_channels, in_channels*2, 3),\n",
    "            nn.Conv2d(in_channels*2, (num_classes+5)*3, 1)\n",
    "        )\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pred(x)\n",
    "        output = output.view(x.size(0), 3, self.num_classes+5, x.size(2), x.size(3))\n",
    "        output = output.permute(0, 1, 3, 4, 2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "febfb77d-ab9d-4b82-91fe-7d4565ab6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolov3(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.darknet53 = DarkNet53()\n",
    "\n",
    "        self.fpn_feature_block1 = FPN_featureBlock(1024, 512)\n",
    "        self.detectionlayer1 = DetectionLayer(512, num_classes)\n",
    "        self.upsampling1 = UpSampling(512, 256)\n",
    "\n",
    "        self.fpn_feature_block2 = FPN_featureBlock(512+256, 256)\n",
    "        self.detectionlayer2 = DetectionLayer(256, num_classes)\n",
    "        self.upsampling2 = UpSampling(256, 128)\n",
    "        \n",
    "        self.fpn_feature_block3 = FPN_featureBlock(256+128, 128)\n",
    "        self.detectionlayer3 = DetectionLayer(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.feature1, self.feature2, self.feature3 = self.darknet53(x)\n",
    "        \n",
    "        x = self.fpn_feature_block1(self.feature3)\n",
    "        output1 = self.detectionlayer1(x)\n",
    "        x = self.upsampling1(x)\n",
    "\n",
    "        x = self.fpn_feature_block2(torch.cat([x, self.feature2], dim=1))\n",
    "        output2 = self.detectionlayer2(x)\n",
    "        x = self.upsampling2(x)\n",
    "\n",
    "        x = self.fpn_feature_block3(torch.cat([x, self.feature1], dim=1))\n",
    "        output3 = self.detectionlayer3(x)\n",
    "\n",
    "        return output1, output2, output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f57f988f-b3ea-42f0-85b6-5c5d7b5a0bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 13, 13, 25])\n",
      "torch.Size([1, 3, 26, 26, 25])\n",
      "torch.Size([1, 3, 52, 52, 25])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((1, 3, 416, 416))\n",
    "model = Yolov3(num_classes = 20)\n",
    "out = model(x)\n",
    "print(out[0].shape) # torch.Size([1, 3, 13, 13, 25])\n",
    "print(out[1].shape) # torch.Size([1, 3, 26, 26, 25])\n",
    "print(out[2].shape) # torch.Size([1, 3, 52, 52, 25]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5ca1fd9-9743-4121-975d-9fe613067b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "Yolov3                                                       [2, 3, 13, 13, 25]        --\n",
       "├─DarkNet53: 1-1                                             [2, 256, 52, 52]          --\n",
       "│    └─BasicConvBlock: 2-1                                   [2, 32, 416, 416]         --\n",
       "│    │    └─Sequential: 3-1                                  [2, 32, 416, 416]         928\n",
       "│    └─Sequential: 2-2                                       [2, 64, 208, 208]         --\n",
       "│    │    └─BasicConvBlock: 3-2                              [2, 64, 208, 208]         18,560\n",
       "│    │    └─ResidualBlock: 3-3                               [2, 64, 208, 208]         20,672\n",
       "│    └─Sequential: 2-3                                       [2, 128, 104, 104]        --\n",
       "│    │    └─BasicConvBlock: 3-4                              [2, 128, 104, 104]        73,984\n",
       "│    │    └─Sequential: 3-5                                  [2, 128, 104, 104]        164,608\n",
       "│    └─Sequential: 2-4                                       [2, 256, 52, 52]          --\n",
       "│    │    └─BasicConvBlock: 3-6                              [2, 256, 52, 52]          295,424\n",
       "│    │    └─Sequential: 3-7                                  [2, 256, 52, 52]          2,627,584\n",
       "│    └─Sequential: 2-5                                       [2, 512, 26, 26]          --\n",
       "│    │    └─BasicConvBlock: 3-8                              [2, 512, 26, 26]          1,180,672\n",
       "│    │    └─Sequential: 3-9                                  [2, 512, 26, 26]          10,498,048\n",
       "│    └─Sequential: 2-6                                       [2, 1024, 13, 13]         --\n",
       "│    │    └─BasicConvBlock: 3-10                             [2, 1024, 13, 13]         4,720,640\n",
       "│    │    └─Sequential: 3-11                                 [2, 1024, 13, 13]         20,983,808\n",
       "├─FPN_featureBlock: 1-2                                      [2, 512, 13, 13]          --\n",
       "│    └─Sequential: 2-7                                       [2, 512, 13, 13]          --\n",
       "│    │    └─BasicConvBlock: 3-12                             [2, 512, 13, 13]          525,312\n",
       "│    │    └─BasicConvBlock: 3-13                             [2, 1024, 13, 13]         4,720,640\n",
       "│    │    └─BasicConvBlock: 3-14                             [2, 512, 13, 13]          525,312\n",
       "│    │    └─BasicConvBlock: 3-15                             [2, 1024, 13, 13]         4,720,640\n",
       "│    │    └─BasicConvBlock: 3-16                             [2, 512, 13, 13]          525,312\n",
       "├─DetectionLayer: 1-3                                        [2, 3, 13, 13, 25]        --\n",
       "│    └─Sequential: 2-8                                       [2, 75, 13, 13]           --\n",
       "│    │    └─BasicConvBlock: 3-17                             [2, 1024, 13, 13]         4,720,640\n",
       "│    │    └─Conv2d: 3-18                                     [2, 75, 13, 13]           76,875\n",
       "├─UpSampling: 1-4                                            [2, 256, 26, 26]          --\n",
       "│    └─Sequential: 2-9                                       [2, 256, 26, 26]          --\n",
       "│    │    └─BasicConvBlock: 3-19                             [2, 256, 13, 13]          131,584\n",
       "│    │    └─Upsample: 3-20                                   [2, 256, 26, 26]          --\n",
       "├─FPN_featureBlock: 1-5                                      [2, 256, 26, 26]          --\n",
       "│    └─Sequential: 2-10                                      [2, 256, 26, 26]          --\n",
       "│    │    └─BasicConvBlock: 3-21                             [2, 256, 26, 26]          197,120\n",
       "│    │    └─BasicConvBlock: 3-22                             [2, 512, 26, 26]          1,180,672\n",
       "│    │    └─BasicConvBlock: 3-23                             [2, 256, 26, 26]          131,584\n",
       "│    │    └─BasicConvBlock: 3-24                             [2, 512, 26, 26]          1,180,672\n",
       "│    │    └─BasicConvBlock: 3-25                             [2, 256, 26, 26]          131,584\n",
       "├─DetectionLayer: 1-6                                        [2, 3, 26, 26, 25]        --\n",
       "│    └─Sequential: 2-11                                      [2, 75, 26, 26]           --\n",
       "│    │    └─BasicConvBlock: 3-26                             [2, 512, 26, 26]          1,180,672\n",
       "│    │    └─Conv2d: 3-27                                     [2, 75, 26, 26]           38,475\n",
       "├─UpSampling: 1-7                                            [2, 128, 52, 52]          --\n",
       "│    └─Sequential: 2-12                                      [2, 128, 52, 52]          --\n",
       "│    │    └─BasicConvBlock: 3-28                             [2, 128, 26, 26]          33,024\n",
       "│    │    └─Upsample: 3-29                                   [2, 128, 52, 52]          --\n",
       "├─FPN_featureBlock: 1-8                                      [2, 128, 52, 52]          --\n",
       "│    └─Sequential: 2-13                                      [2, 128, 52, 52]          --\n",
       "│    │    └─BasicConvBlock: 3-30                             [2, 128, 52, 52]          49,408\n",
       "│    │    └─BasicConvBlock: 3-31                             [2, 256, 52, 52]          295,424\n",
       "│    │    └─BasicConvBlock: 3-32                             [2, 128, 52, 52]          33,024\n",
       "│    │    └─BasicConvBlock: 3-33                             [2, 256, 52, 52]          295,424\n",
       "│    │    └─BasicConvBlock: 3-34                             [2, 128, 52, 52]          33,024\n",
       "├─DetectionLayer: 1-9                                        [2, 3, 52, 52, 25]        --\n",
       "│    └─Sequential: 2-14                                      [2, 75, 52, 52]           --\n",
       "│    │    └─BasicConvBlock: 3-35                             [2, 256, 52, 52]          295,424\n",
       "│    │    └─Conv2d: 3-36                                     [2, 75, 52, 52]           19,275\n",
       "==============================================================================================================\n",
       "Total params: 61,626,049\n",
       "Trainable params: 61,626,049\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 65.43\n",
       "==============================================================================================================\n",
       "Input size (MB): 4.15\n",
       "Forward/backward pass size (MB): 1229.50\n",
       "Params size (MB): 246.50\n",
       "Estimated Total Size (MB): 1480.15\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size = (1, 3, 416, 416), device = \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
