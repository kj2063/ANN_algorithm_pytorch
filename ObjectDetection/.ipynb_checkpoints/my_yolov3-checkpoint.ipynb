{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde6c3e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      2\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\__init__.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\__init__.py:132\u001b[0m\n\u001b[0;32m    130\u001b[0m is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 132\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mkernel32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     last_error \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mget_last_error()\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d1075-432f-4150-9cdb-a8cbb28c634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "IMAGE_SIZE = 416\n",
    "BATCH_SIZE = 10\n",
    "data_dir = '../data/IndoorObjectsDetection'\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "#patience = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd66c9-bed4-49eb-b9bc-27d2ee786742",
   "metadata": {},
   "source": [
    "# **Data Load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e5acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import random\n",
    "\n",
    "data_config = open( data_dir + '/data.yaml')\n",
    "\n",
    "data_info = yaml.load(data_config, Loader=yaml.FullLoader)\n",
    "\n",
    "train_data_path = data_info['train_data_path']\n",
    "val_data_path = data_info['val_data_path']\n",
    "test_data_path = data_info['test_data_path']\n",
    "\n",
    "train_labels_path = data_info['train_labels_path']\n",
    "val_labels_path = data_info['val_labels_path']\n",
    "test_labels_path = data_info['test_labels_path']\n",
    "\n",
    "target_list = data_info['names']\n",
    "target_dict = dict(zip(range(len(target_list)), target_list))\n",
    "\n",
    "num_classes = len(target_list)\n",
    "\n",
    "target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a21b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained = False)\n",
    "layers = [m for m in resnet18.children()]\n",
    "\n",
    "# 마지막 2층인 average pooling & fully connected layer 은 back bone으로 사용하지 않음\n",
    "test_net = nn.Sequential(*layers[:-2]) \n",
    "\n",
    "temp_x = torch.randn(1,3,IMAGE_SIZE,IMAGE_SIZE)\n",
    "temp_y = test_net(temp_x)\n",
    "\n",
    "\n",
    "print(type(temp_x))\n",
    "print(temp_x.shape)\n",
    "print(temp_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1e79dd-dd22-4d2e-956e-fca547da2e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHORS = [[[0.2788, 0.2163],\n",
    "         [0.3750, 0.4760],\n",
    "         [0.8966, 0.7837]],\n",
    "\n",
    "        [[0.0721, 0.1466],\n",
    "         [0.1490, 0.1082],\n",
    "         [0.1418, 0.2861]],\n",
    "\n",
    "        [[0.0240, 0.0312],\n",
    "         [0.0385, 0.0721],\n",
    "         [0.0793, 0.0553]]]\n",
    "\n",
    "GRID_SIZE = [13, 26, 52] \n",
    "\n",
    "scaled_anchors = torch.tensor(ANCHORS)*torch.tensor(GRID_SIZE).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02020a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detection_dataset():\n",
    "    def __init__(self, data_dir, phase, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.phase = phase\n",
    "        self.image_files = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        for fn in os.listdir(os.path.join(self.data_dir, phase, 'images')):\n",
    "            bboxes, class_ids = self.get_label(fn)\n",
    "                                \n",
    "            if(fn.endswith(\"jpg\") and bboxes.size != 0 and class_ids.size != 0):\n",
    "                self.image_files.append(fn)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename, image = self.get_image(index)\n",
    "        bboxes, class_ids = self.get_label(filename)\n",
    "        \n",
    "        if self.transform: \n",
    "            transformed_data = self.transform(image=image, bboxes=bboxes, class_ids=class_ids)\n",
    "            image = transformed_data['image']\n",
    "            bboxes = np.array(transformed_data['bboxes'])\n",
    "            class_ids = np.array(transformed_data['class_ids'])\n",
    "        else:\n",
    "            #transform 을 하지 않을경우 reshape to (C,W,H)\n",
    "            image = torch.Tensor(image).permute(2,0,1)\n",
    "        \n",
    "        target = np.concatenate((bboxes, class_ids[:, np.newaxis]), axis=1)\n",
    "\n",
    "        scaled_anchors_target = self.convert_target_to_scaled_anchors_target(target, ANCHORS)\n",
    "        \n",
    "        return image, scaled_anchors_target, target, filename\n",
    "    \n",
    "    def get_image(self, index):\n",
    "        filename = self.image_files[index]\n",
    "        image_path = os.path.join(self.data_dir, self.phase, 'images', filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        return filename, image\n",
    "    \n",
    "    def get_label(self, filename):\n",
    "        image_id = filename.split('.')[0]\n",
    "        label_file_path = os.path.join(self.data_dir, self.phase, 'labels') + '/' + image_id + '.txt'\n",
    "        try:\n",
    "            bbox_df = pd.read_csv(label_file_path, sep=' ', header=None)\n",
    "\n",
    "            # width or height 가 0이면 제거 \n",
    "            bbox_df = bbox_df[(bbox_df[3] != 0) & (bbox_df[4] != 0)]\n",
    "            \n",
    "            bboxes = np.asarray(bbox_df[[1,2,3,4]])\n",
    "            class_ids = np.asarray(bbox_df[0])\n",
    "            \n",
    "        except Exception as e:\n",
    "            bboxes = np.array([])\n",
    "            class_ids = np.array([])\n",
    "            \n",
    "        return bboxes, class_ids\n",
    "\n",
    "    def convert_target_to_scaled_anchors_target(self, bboxes, anchors):\n",
    "\n",
    "        self.grid_sizes = GRID_SIZE\n",
    "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2]) \n",
    "        # Number of anchor boxes  \n",
    "        self.num_anchors = self.anchors.shape[0] \n",
    "        # Number of anchor boxes per scale \n",
    "        self.num_anchors_per_scale = self.num_anchors // 3\n",
    "        # Ignore IoU threshold \n",
    "        self.ignore_iou_thresh = 0.5\n",
    "\n",
    "        \n",
    "        # Below assumes 3 scale predictions (as paper) and same num of anchors per scale \n",
    "        # target : [probabilities, x, y, width, height, class_label] \n",
    "        targets = [torch.zeros((self.num_anchors_per_scale, s, s, 6)) \n",
    "                   for s in self.grid_sizes] \n",
    "          \n",
    "        # Identify anchor box and cell for each bounding box \n",
    "        for box in bboxes: \n",
    "            # Calculate iou of bounding box with anchor boxes \n",
    "            iou_anchors = self.iou_WH(torch.tensor(box[2:4]), self.anchors)\n",
    "\n",
    "            # Selecting the best anchor box \n",
    "            anchor_indices = iou_anchors.argsort(descending=True, dim=0) \n",
    "            x, y, width, height, class_label = box \n",
    "  \n",
    "            # At each scale, assigning the bounding box to the  \n",
    "            # best matching anchor box \n",
    "            has_anchor = [False] * 3\n",
    "            for anchor_idx in anchor_indices: \n",
    "                scale_idx = anchor_idx // self.num_anchors_per_scale \n",
    "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale \n",
    "                  \n",
    "                # Identifying the grid size for the scale \n",
    "                s = self.grid_sizes[scale_idx] \n",
    "                  \n",
    "                # Identifying the cell to which the bounding box belongs \n",
    "                i, j = int(s * y), int(s * x) \n",
    "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0] \n",
    "                  \n",
    "                # Check if the anchor box is already assigned \n",
    "                if not anchor_taken and not has_anchor[scale_idx]: \n",
    "                    # Set the probability to 1 \n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
    "  \n",
    "                    # Calculating the center of the bounding box relative \n",
    "                    # to the cell \n",
    "                    x_cell, y_cell = s * x - j, s * y - i  \n",
    "  \n",
    "                    # Calculating the width and height of the bounding box  \n",
    "                    # relative to the cell \n",
    "                    width_cell, height_cell = (width * s, height * s) \n",
    "  \n",
    "                    # Idnetify the box coordinates \n",
    "                    box_coordinates = torch.tensor( \n",
    "                                        [x_cell, y_cell, width_cell,  \n",
    "                                         height_cell] \n",
    "                                    ) \n",
    "  \n",
    "                    # Assigning the box coordinates to the target \n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates \n",
    "  \n",
    "                    # Assigning the class label to the target \n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label) \n",
    "  \n",
    "                    # Set the anchor box as assigned for the scale \n",
    "                    has_anchor[scale_idx] = True\n",
    "  \n",
    "                # If the anchor box is already assigned, check if the  \n",
    "                # IoU is greater than the threshold \n",
    "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
    "                    # Set the probability to -1 to ignore the anchor box \n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1\n",
    "\n",
    "        return tuple(targets)\n",
    "\n",
    "    def iou_WH(self, box1, box2):\n",
    "        # IoU score based on width and height of bounding boxes \n",
    "          \n",
    "        # Calculate intersection area \n",
    "        intersection_area = torch.min(box1[..., 0], box2[..., 0]) * torch.min(box1[..., 1], box2[..., 1]) \n",
    "  \n",
    "        # Calculate union area \n",
    "        box1_area = box1[..., 0] * box1[..., 1] \n",
    "        box2_area = box2[..., 0] * box2[..., 1] \n",
    "        union_area = box1_area + box2_area - intersection_area \n",
    "  \n",
    "        # Calculate IoU score \n",
    "        iou_score = intersection_area / union_area \n",
    "  \n",
    "        # Return IoU score \n",
    "        return iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcf18cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\"\"\"\n",
    "    when you use yolo format bbox param\n",
    "    need to add logic in albumentations/augmentations/bbox_utils.py - check_bbox() method\n",
    "    to make bbox boundery in [0,1]\n",
    "    \n",
    "    -------------------\n",
    "    bbox=list(bbox)\n",
    "    \n",
    "    for i in range(4):\n",
    "      if (bbox[i]<0) :\n",
    "        bbox[i]=0\n",
    "      elif (bbox[i]>1) :\n",
    "        bbox[i]=1\n",
    "    \n",
    "    bbox=tuple(bbox)\n",
    "    --------------------\n",
    "\"\"\"\n",
    "\n",
    "#mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225) -> imageNet 데이터셋에 기반한 계산된 수치 \n",
    "transform = A.Compose([\n",
    "        A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "        #A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    ")\n",
    "\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0c4207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "from my_util import set_bounding_boxes, set_bounding_box, get_random_color_dict\n",
    "from ipywidgets import interact\n",
    "\n",
    "transformed_train_dataset = Detection_dataset(data_dir=data_dir, phase=\"train\", transform=transform)\n",
    "\n",
    "@interact(index=(0, len(transformed_train_dataset)-1))\n",
    "def show_transformed_image(index=0):\n",
    "    img, scaled_anchors_target, target, filename = transformed_train_dataset[index]\n",
    "\n",
    "    np_image = make_grid(img, normalize=False).permute(1,2,0).numpy()\n",
    "    res = set_bounding_boxes(np_image, target[:,0:4], 'yolo', target[:,4].astype(int), target_dict, get_random_color_dict(target_dict))\n",
    "    \n",
    "    # np_image = make_grid(img, normalize=True).permute(1,2,0).numpy()\n",
    "    # np_image_unit8 = (np_image*255).astype(np.uint8)\n",
    "    # res = set_bounding_boxes(np_image_unit8, target[:,0:4], 'yolo', target[:,4].astype(int), target_dict, get_random_color_dict(target_dict))\n",
    "    \n",
    "    plt.imshow(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd5f706-5d39-4b15-a091-e227aa2a11bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_dataset[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe4aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image_list = []\n",
    "    scaled_anchors_target_list = []\n",
    "    target_list = []\n",
    "    filename_list = []\n",
    "    \n",
    "    for a,b,c,d in batch:\n",
    "        image_list.append(a)\n",
    "        scaled_anchors_target_list.append(b)\n",
    "        target_list.append(c)\n",
    "        filename_list.append(d)\n",
    "        \n",
    "    return torch.stack(image_list, dim=0), scaled_anchors_target_list, target_list, filename_list\n",
    "\n",
    "def train_valid_dataloader(data_dir, batch_size=4, transform=None):\n",
    "    dataloaders = {}\n",
    "    \n",
    "    train_dataset = Detection_dataset(data_dir=data_dir, phase=\"train\", transform=transform)\n",
    "    dataloaders[\"train\"] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    valid_dataset = Detection_dataset(data_dir=data_dir, phase=\"valid\", transform=transform)    \n",
    "    dataloaders[\"valid\"] = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    return dataloaders\n",
    "\n",
    "dataloaders = train_valid_dataloader(data_dir, BATCH_SIZE, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119930c0-3296-41da-a217-3d6fa5e4ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pip install --trusted-host pypi.python.org --trusted-host pypi.org --trusted-host files.pythonhosted.org tqdm\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb81364-0e19-48b0-a177-41bdd67bad15",
   "metadata": {},
   "source": [
    "# **YOLO_V3 Model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e935a945-f31e-489f-9eac-8d3ee0cdbd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=(kernel_size-1)//2, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34287db5-5f0c-4869-aafd-fcefd826848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backbone\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            BasicConvBlock(channels, channels//2, kernel_size=1, stride=1),\n",
    "            BasicConvBlock(channels//2, channels, kernel_size=3, stride=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.residual(x) + x\n",
    "\n",
    "class DarkNet53(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = BasicConvBlock(3, 32, 3, 1)\n",
    "        self.block1 = nn.Sequential(\n",
    "            BasicConvBlock(32, 64, 3, 2),\n",
    "            ResidualBlock(64)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            BasicConvBlock(64, 128, 3, 2),\n",
    "            nn.Sequential(*[ResidualBlock(128) for _ in range(2)])\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            BasicConvBlock(128, 256, 3, 2),\n",
    "            nn.Sequential(*[ResidualBlock(256) for _ in range(8)])\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "            BasicConvBlock(256, 512, 3, 2),\n",
    "            nn.Sequential(*[ResidualBlock(512) for _ in range(8)])\n",
    "        )\n",
    "        self.block5 = nn.Sequential(\n",
    "            BasicConvBlock(512, 1024, 3, 2),\n",
    "            nn.Sequential(*[ResidualBlock(1024) for _ in range(4)])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        feature_map1 = self.block3(x)\n",
    "        feature_map2 = self.block4(feature_map1)\n",
    "        feature_map3 = self.block5(feature_map2)\n",
    "\n",
    "        return feature_map1, feature_map2, feature_map3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d4016d-ce05-4a19-acac-55cf19ce00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neck : FPN top-down\n",
    "class FPN_featureBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            BasicConvBlock(in_channels, out_channels, 1),\n",
    "            BasicConvBlock(out_channels, out_channels*2, 3),\n",
    "            BasicConvBlock(out_channels*2, out_channels, 1),\n",
    "            BasicConvBlock(out_channels, out_channels*2, 3),\n",
    "            BasicConvBlock(out_channels*2, out_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "        \n",
    "class UpSampling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            BasicConvBlock(in_channels, out_channels, 1),\n",
    "            nn.Upsample(scale_factor = 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bb601a-7291-4563-8f78-a816716d2a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Head\n",
    "class DetectionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, img_size=IMAGE_SIZE):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = 3\n",
    "        \n",
    "        #(4+1+num_classes)*3 -> [4(x,y,w,h) + 1(Objectness Score) + num_classes(Class Probabilities)] * 3(number of anchors)\n",
    "        self.pred = nn.Sequential(\n",
    "            BasicConvBlock(in_channels, in_channels*2, 3),\n",
    "            nn.Conv2d(in_channels*2, (4+1+num_classes)*3, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        grid_size = x.size(2)\n",
    "        \n",
    "        output = self.pred(x)\n",
    "        output = output.view(batch_size, self.num_anchors, 4+1+self.num_classes, grid_size, grid_size) \n",
    "        output = output.permute(0, 1, 3, 4, 2)\n",
    "        \n",
    "        output = output.contiguous()\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febfb77d-ab9d-4b82-91fe-7d4565ab6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolov3(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.darknet53 = DarkNet53()\n",
    "\n",
    "        self.fpn_feature_block1 = FPN_featureBlock(1024, 512)\n",
    "        self.detectionlayer1 = DetectionLayer(512, num_classes)\n",
    "        self.upsampling1 = UpSampling(512, 256)\n",
    "\n",
    "        self.fpn_feature_block2 = FPN_featureBlock(512+256, 256)\n",
    "        self.detectionlayer2 = DetectionLayer(256, num_classes)\n",
    "        self.upsampling2 = UpSampling(256, 128)\n",
    "        \n",
    "        self.fpn_feature_block3 = FPN_featureBlock(256+128, 128)\n",
    "        self.detectionlayer3 = DetectionLayer(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.feature1, self.feature2, self.feature3 = self.darknet53(x)\n",
    "        \n",
    "        x = self.fpn_feature_block1(self.feature3)\n",
    "        output1 = self.detectionlayer1(x)\n",
    "        x = self.upsampling1(x)\n",
    "\n",
    "        x = self.fpn_feature_block2(torch.cat([x, self.feature2], dim=1))\n",
    "        output2 = self.detectionlayer2(x)\n",
    "        x = self.upsampling2(x)\n",
    "\n",
    "        x = self.fpn_feature_block3(torch.cat([x, self.feature1], dim=1))\n",
    "        output3 = self.detectionlayer3(x)\n",
    "\n",
    "        return output1, output2, output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f988f-b3ea-42f0-85b6-5c5d7b5a0bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Yolov3(num_classes).to(device)\n",
    "\n",
    "'''\n",
    "x = torch.randint(0, 255, (1, 3, 416, 416)).float()\n",
    "out = model(x)\n",
    "print(out[0].shape) # torch.Size([1, 3, 13, 13, 15]) -> 3*13*13 = 507 개의 bounding box 예측\n",
    "print(out[1].shape) # torch.Size([1, 3, 26, 26, 15]) -> 3*26*26 = 2028 개의 bounding box 예측\n",
    "print(out[2].shape) # torch.Size([1, 3, 52, 52, 25]) -> 3*52*52 = 8112 개의 bounding box 예측\n",
    "\n",
    "# 총 10647개의 bounding box 예측\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca1fd9-9743-4121-975d-9fe613067b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size = (1, 3, 416, 416), device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a9e318-6f8c-4a08-bd36-be4ca323133c",
   "metadata": {},
   "source": [
    "# **YOLO_V3 Loss** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513a81f-746d-4919-8f90-e62de2f7eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining YOLO loss class \n",
    "class YOLOLoss(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__() \n",
    "        self.mse = nn.MSELoss() \n",
    "        self.bce = nn.BCEWithLogitsLoss() \n",
    "        self.cross_entropy = nn.CrossEntropyLoss() \n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "\n",
    "    # Defining a function to calculate Intersection over Union (IoU) \n",
    "    def iou(self, box1, box2): \n",
    "        # IoU score for prediction and label \n",
    "        # box1 (prediction) and box2 (label) are both in [x, y, width, height] format \n",
    "          \n",
    "        # Box coordinates of prediction \n",
    "        b1_x1 = box1[..., 0:1] - box1[..., 2:3] / 2\n",
    "        b1_y1 = box1[..., 1:2] - box1[..., 3:4] / 2\n",
    "        b1_x2 = box1[..., 0:1] + box1[..., 2:3] / 2\n",
    "        b1_y2 = box1[..., 1:2] + box1[..., 3:4] / 2\n",
    "  \n",
    "        # Box coordinates of ground truth \n",
    "        b2_x1 = box2[..., 0:1] - box2[..., 2:3] / 2\n",
    "        b2_y1 = box2[..., 1:2] - box2[..., 3:4] / 2\n",
    "        b2_x2 = box2[..., 0:1] + box2[..., 2:3] / 2\n",
    "        b2_y2 = box2[..., 1:2] + box2[..., 3:4] / 2\n",
    "  \n",
    "        # Get the coordinates of the intersection rectangle \n",
    "        x1 = torch.max(b1_x1, b2_x1) \n",
    "        y1 = torch.max(b1_y1, b2_y1) \n",
    "        x2 = torch.min(b1_x2, b2_x2) \n",
    "        y2 = torch.min(b1_y2, b2_y2) \n",
    "        # Make sure the intersection is at least 0 \n",
    "        intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0) \n",
    "  \n",
    "        # Calculate the union area \n",
    "        box1_area = abs((b1_x2 - b1_x1) * (b1_y2 - b1_y1)) \n",
    "        box2_area = abs((b2_x2 - b2_x1) * (b2_y2 - b2_y1)) \n",
    "        union = box1_area + box2_area - intersection \n",
    "  \n",
    "        # Calculate the IoU score \n",
    "        epsilon = 1e-6\n",
    "        iou_score = intersection / (union + epsilon) \n",
    "  \n",
    "        # Return IoU score \n",
    "        return iou_score \n",
    "      \n",
    "    def forward(self, pred, target, anchors): \n",
    "        # Identifying which cells in target have objects  \n",
    "        # and which have no objects \n",
    "        obj = target[...,0] == 1\n",
    "        no_obj = target[...,0] == 0\n",
    "  \n",
    "        # Calculating No object loss\n",
    "        no_object_loss = self.bce(pred[..., 0:1][no_obj], \n",
    "                                  target[..., 0:1][no_obj]) \n",
    "          \n",
    "        # Reshaping anchors to match predictions \n",
    "        anchors = anchors.reshape(1, 3, 1, 1, 2) \n",
    "        # Box prediction confidence \n",
    "        box_preds = torch.cat([self.sigmoid(pred[..., 1:3]), \n",
    "                               torch.exp(pred[..., 3:5]) * anchors ],dim=-1) \n",
    "        \n",
    "        # Calculating intersection over union for prediction and target \n",
    "        ious = self.iou(box_preds[obj], target[..., 1:5][obj]).detach() \n",
    "        # Calculating Object loss \n",
    "        object_loss = self.mse(self.sigmoid(pred[..., 0:1][obj]), \n",
    "                               ious * target[..., 0:1][obj]) \n",
    "  \n",
    "          \n",
    "        # Predicted box coordinates \n",
    "        pred[..., 1:3] = self.sigmoid(pred[..., 1:3]) \n",
    "        # Target box coordinates \n",
    "        target[..., 3:5] = torch.log(1e-6 + target[..., 3:5] / anchors) \n",
    "        # Calculating box coordinate loss \n",
    "        box_loss = self.mse(pred[..., 1:5][obj], \n",
    "                            target[..., 1:5][obj]) \n",
    "  \n",
    "          \n",
    "        # Claculating class loss \n",
    "        class_loss = self.cross_entropy((pred[..., 5:][obj]), \n",
    "                                   target[..., 5][obj].long()) \n",
    "        \n",
    "        # Total loss \n",
    "        return ( \n",
    "            box_loss \n",
    "            + object_loss \n",
    "            + no_object_loss \n",
    "            + class_loss \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa26fcf-f699-453f-87db-c5b20dbfbb30",
   "metadata": {},
   "source": [
    "# **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1bcad4-e874-4a32-b96f-832cf0f6fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Define the train function to train the model \n",
    "def training_loop(loader, model, optimizer, loss_fn, scaler, scaled_anchors): \n",
    "    # Creating a progress bar \n",
    "    progress_bar = tqdm(loader[\"train\"], leave=True) \n",
    "  \n",
    "    # Initializing a list to store the losses \n",
    "    train_losses = []\n",
    "    valid_losses = [] \n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    # Iterating over the training data \n",
    "    for _, (tx, ty, _, _) in enumerate(progress_bar): \n",
    "        tx = tx.to(torch.float32).to(device)\n",
    "        ty0 = torch.stack([t[0] for t in ty])\n",
    "        ty1 = torch.stack([t[1] for t in ty])\n",
    "        ty2 = torch.stack([t[2] for t in ty])\n",
    "\n",
    "        with torch.cuda.amp.autocast(): \n",
    "            # Getting the model predictions \n",
    "            train_outputs = model(tx) \n",
    "            # Calculating the loss at each scale\n",
    "            train_loss = ( \n",
    "                  loss_fn(train_outputs[0], ty0, scaled_anchors[0]) \n",
    "                + loss_fn(train_outputs[1], ty1, scaled_anchors[1]) \n",
    "                + loss_fn(train_outputs[2], ty2, scaled_anchors[2]) \n",
    "            ) \n",
    "  \n",
    "        # Add the loss to the list \n",
    "        train_losses.append(train_loss.item()) \n",
    "  \n",
    "        # Reset gradients \n",
    "        optimizer.zero_grad() \n",
    "  \n",
    "        # Backpropagate the loss \n",
    "        scaler.scale(train_loss).backward() \n",
    "  \n",
    "        # Optimization step \n",
    "        scaler.step(optimizer) \n",
    "  \n",
    "        # Update the scaler for next iteration \n",
    "        scaler.update() \n",
    "  \n",
    "        # update progress bar with loss \n",
    "        mean_train_loss = sum(train_losses) / len(train_losses) \n",
    "        progress_bar.set_postfix(train_loss=mean_train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (vx, vy, _, _) in loader[\"valid\"]:\n",
    "            vx = vx.to(torch.float32).to(device)\n",
    "            vy0 = torch.stack([t[0] for t in vy])\n",
    "            vy1 = torch.stack([t[1] for t in vy])\n",
    "            vy2 = torch.stack([t[2] for t in vy])\n",
    "      \n",
    "            with torch.cuda.amp.autocast(): \n",
    "                # Getting the model predictions \n",
    "                valid_outputs = model(vx) \n",
    "                # Calculating the loss at each scale \n",
    "                valid_loss = ( \n",
    "                      loss_fn(valid_outputs[0], vy0, scaled_anchors[0]) \n",
    "                    + loss_fn(valid_outputs[1], vy1, scaled_anchors[1]) \n",
    "                    + loss_fn(valid_outputs[2], vy2, scaled_anchors[2]) \n",
    "                ) \n",
    "      \n",
    "            # Add the loss to the list \n",
    "            valid_losses.append(valid_loss.item())\n",
    "\n",
    "            # update progress bar with loss \n",
    "            mean_valid_loss = sum(valid_losses) / len(valid_losses)\n",
    "            progress_bar.set_postfix(valid_loss=mean_valid_loss)\n",
    "\n",
    "    return mean_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a5a05-0a05-41eb-997f-a498930ce237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save checkpoint \n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"): \n",
    "    print(\"==> Saving checkpoint\") \n",
    "    checkpoint = { \n",
    "        \"state_dict\": model.state_dict(), \n",
    "        \"optimizer\": optimizer.state_dict(), \n",
    "    } \n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "# Function to load checkpoint \n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr): \n",
    "    print(\"==> Loading checkpoint\") \n",
    "    checkpoint = torch.load(checkpoint_file, map_location=device) \n",
    "    model.load_state_dict(checkpoint[\"state_dict\"]) \n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"]) \n",
    "  \n",
    "    for param_group in optimizer.param_groups: \n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0578cc5-324d-4ab2-8e23-be29b6a89af6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "\n",
    "# Defining the optimizer \n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate) \n",
    "  \n",
    "# Defining the loss function \n",
    "loss_fn = YOLOLoss() \n",
    "  \n",
    "# Defining the scaler for mixed precision training \n",
    "scaler = torch.cuda.amp.GradScaler() \n",
    "\n",
    "mean_valid_losses = []\n",
    "\n",
    "for e in range(1, epochs+1): \n",
    "    print(\"Epoch:\", e) \n",
    "    mean_valid_loss = training_loop(dataloaders, model, optimizer, loss_fn, scaler, scaled_anchors) \n",
    "\n",
    "    mean_valid_losses.append(mean_valid_loss)\n",
    "\n",
    "    #Saving the model \n",
    "    save_checkpoint(model, optimizer, filename=f\"my_yolov3_checkpoint_epoch{e}.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354594c0-68f4-4721-87dc-8f1edd562080",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b7af4a-fd61-40cd-b15f-86f94b4260f1",
   "metadata": {},
   "source": [
    "# **Testing Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ee410-daa4-4023-97a3-f77e5ce5f413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2): \n",
    "        # IoU score for prediction and label \n",
    "        # box1 (prediction) and box2 (label) are both in [x, y, width, height] format \n",
    "          \n",
    "        # Box coordinates of prediction \n",
    "        b1_x1 = box1[..., 0:1] - box1[..., 2:3] / 2\n",
    "        b1_y1 = box1[..., 1:2] - box1[..., 3:4] / 2\n",
    "        b1_x2 = box1[..., 0:1] + box1[..., 2:3] / 2\n",
    "        b1_y2 = box1[..., 1:2] + box1[..., 3:4] / 2\n",
    "  \n",
    "        # Box coordinates of ground truth \n",
    "        b2_x1 = box2[..., 0:1] - box2[..., 2:3] / 2\n",
    "        b2_y1 = box2[..., 1:2] - box2[..., 3:4] / 2\n",
    "        b2_x2 = box2[..., 0:1] + box2[..., 2:3] / 2\n",
    "        b2_y2 = box2[..., 1:2] + box2[..., 3:4] / 2\n",
    "  \n",
    "        # Get the coordinates of the intersection rectangle \n",
    "        x1 = torch.max(b1_x1, b2_x1) \n",
    "        y1 = torch.max(b1_y1, b2_y1) \n",
    "        x2 = torch.min(b1_x2, b2_x2) \n",
    "        y2 = torch.min(b1_y2, b2_y2) \n",
    "        # Make sure the intersection is at least 0 \n",
    "        intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0) \n",
    "  \n",
    "        # Calculate the union area \n",
    "        box1_area = abs((b1_x2 - b1_x1) * (b1_y2 - b1_y1)) \n",
    "        box2_area = abs((b2_x2 - b2_x1) * (b2_y2 - b2_y1)) \n",
    "        union = box1_area + box2_area - intersection \n",
    "  \n",
    "        # Calculate the IoU score \n",
    "        epsilon = 1e-6\n",
    "        iou_score = intersection / (union + epsilon) \n",
    "  \n",
    "        # Return IoU score \n",
    "        return iou_score \n",
    "\n",
    "# Non-maximum suppression function to remove overlapping bounding boxes \n",
    "def nms(bboxes, iou_threshold, threshold): \n",
    "    # Filter out bounding boxes with confidence below the threshold. \n",
    "    bboxes = [box for box in bboxes if box[1] > threshold] \n",
    "  \n",
    "    # Sort the bounding boxes by confidence in descending order. \n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True) \n",
    "  \n",
    "    # Initialize the list of bounding boxes after non-maximum suppression. \n",
    "    bboxes_nms = [] \n",
    "  \n",
    "    while bboxes: \n",
    "        # Get the first bounding box. \n",
    "        first_box = bboxes.pop(0) \n",
    "  \n",
    "        # Iterate over the remaining bounding boxes. \n",
    "        for box in bboxes: \n",
    "        # If the bounding boxes do not overlap or if the first bounding box has \n",
    "        # a higher confidence, then add the second bounding box to the list of \n",
    "        # bounding boxes after non-maximum suppression. \n",
    "            if box[0] != first_box[0] or iou( \n",
    "                torch.tensor(first_box[2:]), \n",
    "                torch.tensor(box[2:]), \n",
    "            ) < iou_threshold: \n",
    "                # Check if box is not in bboxes_nms \n",
    "                if box not in bboxes_nms: \n",
    "                    # Add box to bboxes_nms \n",
    "                    bboxes_nms.append(box) \n",
    "  \n",
    "    # Return bounding boxes after non-maximum suppression. \n",
    "    return bboxes_nms\n",
    "\n",
    "# Function to convert cells to bounding boxes \n",
    "def convert_cells_to_bboxes(predictions, anchors, s, is_predictions=True): \n",
    "    # Batch size used on predictions \n",
    "    batch_size = predictions.shape[0] \n",
    "    # Number of anchors \n",
    "    num_anchors = len(anchors) \n",
    "    # List of all the predictions \n",
    "    box_predictions = predictions[..., 1:5] \n",
    "  \n",
    "    # If the input is predictions then we will pass the x and y coordinate \n",
    "    # through sigmoid function and width and height to exponent function and \n",
    "    # calculate the score and best class. \n",
    "    if is_predictions: \n",
    "        anchors = anchors.reshape(1, len(anchors), 1, 1, 2) \n",
    "        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2]) \n",
    "        box_predictions[..., 2:] = torch.exp( \n",
    "            box_predictions[..., 2:]) * anchors \n",
    "        scores = torch.sigmoid(predictions[..., 0:1]) \n",
    "        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1) \n",
    "      \n",
    "    # Else we will just calculate scores and best class. \n",
    "    else: \n",
    "        scores = predictions[..., 0:1] \n",
    "        best_class = predictions[..., 5:6] \n",
    "  \n",
    "    # Calculate cell indices \n",
    "    cell_indices = ( \n",
    "        torch.arange(s) \n",
    "        .repeat(predictions.shape[0], 3, s, 1) \n",
    "        .unsqueeze(-1) \n",
    "        .to(predictions.device) \n",
    "    ) \n",
    "  \n",
    "    # Calculate x, y, width and height with proper scaling \n",
    "    x = 1 / s * (box_predictions[..., 0:1] + cell_indices) \n",
    "    y = 1 / s * (box_predictions[..., 1:2] +\n",
    "                 cell_indices.permute(0, 1, 3, 2, 4)) \n",
    "    width_height = 1 / s * box_predictions[..., 2:4] \n",
    "  \n",
    "    # Concatinating the values and reshaping them in \n",
    "    # (BATCH_SIZE, num_anchors * S * S, 6) shape \n",
    "    converted_bboxes = torch.cat( \n",
    "        (best_class, scores, x, y, width_height), dim=-1\n",
    "    ).reshape(batch_size, num_anchors * s * s, 6) \n",
    "  \n",
    "    # Returning the reshaped and converted bounding box list \n",
    "    return converted_bboxes.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f06b6-2df8-4e35-a71d-09d1edf0715b",
   "metadata": {},
   "source": [
    "### Get test img file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060bf4c8-635a-480e-9efb-5aad87acccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "checkpoint_file = \"my_yolov3_checkpoint_epoch10.pth.tar\"\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate) \n",
    "\n",
    "load_checkpoint(checkpoint_file, model, optimizer, learning_rate) \n",
    "\n",
    "image_files = []\n",
    "x = []\n",
    "\n",
    "test_transform = A.Compose([\n",
    "        A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "        #A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for idx, fn in enumerate(os.listdir(os.path.join(data_dir, 'test', 'images'))):                \n",
    "    if(fn.endswith(\"png\")) and idx<1:\n",
    "        image_files.append(fn)\n",
    "        \n",
    "for index in range(len(image_files)):\n",
    "    filename = image_files[index]\n",
    "    image_path = os.path.join(data_dir, 'test', 'images', filename)\n",
    "    image = cv2.imread(image_path)\n",
    "    transform_image = test_transform(image=image)\n",
    "    \n",
    "    x.append(transform_image[\"image\"])\n",
    "    \n",
    "x = torch.stack(x).to(torch.float32).to(device)\n",
    "\n",
    "model.eval() \n",
    "with torch.no_grad(): \n",
    "    # Getting the model predictions \n",
    "    output = model(x) \n",
    "    # Getting the bounding boxes from the predictions \n",
    "    bboxes = [[] for _ in range(x.shape[0])] \n",
    "  \n",
    "    # Getting bounding boxes for each scale \n",
    "    for i in range(3): \n",
    "        batch_size, A, S, _, _ = output[i].shape \n",
    "        anchor = scaled_anchors[i] \n",
    "        boxes_scale_i = convert_cells_to_bboxes( \n",
    "                            output[i], anchor, s=S, is_predictions=True\n",
    "                        ) \n",
    "        for idx, (box) in enumerate(boxes_scale_i): \n",
    "            bboxes[idx] += box \n",
    "model.train() \n",
    "\n",
    "y = []\n",
    "\n",
    "# Plotting the image with bounding boxes for each image in the batch \n",
    "for i in range(len(image_files)): \n",
    "    # Applying non-max suppression to remove overlapping bounding boxes \n",
    "    nms_boxes = nms(bboxes[i], iou_threshold=0.5, threshold=0.2) \n",
    "    y.append(nms_boxes)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e27f96-1076-4dd7-af4c-8bca127f8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "\n",
    "@interact(index=(0, len(image_files)-1))\n",
    "def show_transformed_image(index=0):\n",
    "    \n",
    "    np_image = make_grid(x[index], normalize=False).permute(1,2,0).numpy().astype(np.uint8)\n",
    "    res = set_bounding_boxes(np_image, y[index][:,2:6], 'yolo', y[index][:,0].astype(int), target_dict, get_random_color_dict(target_dict))\n",
    "\n",
    "    plt.imshow(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e785259a-1a6f-46b4-893c-a080a63925ec",
   "metadata": {},
   "source": [
    "**Ref.** https://www.geeksforgeeks.org/yolov3-from-scratch-using-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046ed43-2bc4-4a10-b6a1-5f9781c24a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [torch.tensor([[1,2,3],[4,5,6],[7,8,9]]),torch.tensor([[7,8,9],[4,5,6],[1,2,3]])]\n",
    "\n",
    "a = [t[...,0] == 1 for t in a]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e99f9e-b6d0-47e1-be0a-b90d93317e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= range(2)\n",
    "for k in a:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853cdbf-eff3-4c52-b6f9-36ae3fe577b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
